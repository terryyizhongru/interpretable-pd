{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b287b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "def compute_average_report_across_runs(reports):\n",
    "    overall_report = {\n",
    "        'HC': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'PD': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'accuracy': [],\n",
    "        'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "    }\n",
    "\n",
    "    # Spoiler: It's gonna be inefficient :)\n",
    "\n",
    "    for report in reports:\n",
    "        for key in report.keys():\n",
    "            if key == 'accuracy':\n",
    "                overall_report[key].append(report[key])\n",
    "            else:\n",
    "                for key2 in report[key].keys():\n",
    "                    overall_report[key][key2].append(report[key][key2])\n",
    "\n",
    "    for key in overall_report.keys():\n",
    "            if key == 'accuracy':\n",
    "                overall_report[key] = f'{round(np.array(overall_report[key]).mean(), 4)}±{round(np.array(overall_report[key]).std(), 4)}'\n",
    "            else:\n",
    "                for key2 in report[key].keys():\n",
    "                    overall_report[key][key2] = f'{round(np.array(overall_report[key][key2]).mean(), 4)}±{round(np.array(overall_report[key][key2]).std(), 4)}'\n",
    "\n",
    "    # -- just for a more clean output\n",
    "    overall_report = pd.DataFrame.from_dict(overall_report).T\n",
    "    overall_report.iloc[2,0] = ''\n",
    "    overall_report.iloc[2,1] = ''\n",
    "    overall_report.iloc[2,3] = overall_report.iloc[3,3]\n",
    "\n",
    "    return overall_report\n",
    "\n",
    "def get_reports(exps_dir):\n",
    "    val_reports = []\n",
    "    test_reports = []\n",
    "    run_dirs = os.listdir(exps_dir)\n",
    "    for run_dir in run_dirs:\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "\n",
    "        run_dir_path = os.path.join(exps_dir, run_dir)\n",
    "        fold_dirs = os.listdir(run_dir_path)\n",
    "        \n",
    "        for fold_dir in fold_dirs:\n",
    "            model_output_dir = os.path.join(run_dir_path, fold_dir, 'model_output')\n",
    "\n",
    "            # -- validation set\n",
    "            val_report_path = os.path.join(os.path.join(model_output_dir, 'validation_classification.pkl'))\n",
    "            with open(val_report_path, 'rb') as f:\n",
    "                val_model_output = pickle.load(f)\n",
    "\n",
    "            val_preds += val_model_output['preds']\n",
    "            val_labels += val_model_output['labels']\n",
    "\n",
    "            # -- test set\n",
    "            test_report_path = os.path.join(os.path.join(model_output_dir, 'test_classification.pkl'))\n",
    "            with open(test_report_path, 'rb') as f:\n",
    "                test_model_output = pickle.load(f)\n",
    "\n",
    "            test_preds += test_model_output['preds']\n",
    "            test_labels += test_model_output['labels']\n",
    "\n",
    "        # -- computing reports\n",
    "        val_reports.append(\n",
    "            classification_report(\n",
    "                val_labels,\n",
    "                val_preds,\n",
    "                target_names=['HC', 'PD'],\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        test_reports.append(\n",
    "            classification_report(\n",
    "                test_labels,\n",
    "                test_preds,\n",
    "                target_names=['HC', 'PD'],\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return val_reports, test_reports\n",
    "\n",
    "\n",
    "def get_report_perfold(exps_dir):\n",
    "\n",
    "    val_fold_reports = {}\n",
    "    test_fold_reports = {}\n",
    "    run_dirs = sorted(os.listdir(exps_dir))\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        run_dir_path = os.path.join(exps_dir, run_dir)\n",
    "        fold_dirs = sorted(os.listdir(run_dir_path))\n",
    "        \n",
    "        val_fold_reports[run_dir] = []\n",
    "        test_fold_reports[run_dir] = []\n",
    "        \n",
    "        for fold_dir in fold_dirs:\n",
    "            model_output_dir = os.path.join(run_dir_path, fold_dir, 'model_output')\n",
    "            \n",
    "            # -- validation set\n",
    "            val_report_path = os.path.join(model_output_dir, 'validation_classification.pkl')\n",
    "            if os.path.exists(val_report_path):\n",
    "                with open(val_report_path, 'rb') as f:\n",
    "                    val_model_output = pickle.load(f)\n",
    "                \n",
    "                val_probs = [preds[1] for preds in val_model_output['probs']]\n",
    "                val_model_output['probs'] = val_probs  \n",
    "         \n",
    "                val_auc = roc_auc_score(val_model_output['labels'], val_probs)\n",
    "                \n",
    "                val_fold_report = classification_report(\n",
    "                    val_model_output['labels'],\n",
    "                    val_model_output['preds'],\n",
    "                    target_names=['HC', 'PD'],\n",
    "                    output_dict=True,\n",
    "                )\n",
    "                \n",
    "                val_fold_reports[run_dir].append({\n",
    "                    'fold': fold_dir,\n",
    "                    'report': val_fold_report,\n",
    "                    'preds': val_model_output['preds'],\n",
    "                    'labels': val_model_output['labels'],\n",
    "                    'probs': val_model_output['probs'],  \n",
    "                    'auc': val_auc  \n",
    "                })\n",
    "            \n",
    "            # -- test set\n",
    "            test_report_path = os.path.join(model_output_dir, 'test_classification.pkl')\n",
    "            if os.path.exists(test_report_path):\n",
    "                with open(test_report_path, 'rb') as f:\n",
    "                    test_model_output = pickle.load(f)\n",
    "                \n",
    "                test_probs = [preds[1] for preds in test_model_output['probs']]\n",
    "                test_model_output['probs'] = test_probs  \n",
    "                \n",
    "                test_auc = roc_auc_score(test_model_output['labels'], test_probs)\n",
    "                \n",
    "                test_fold_report = classification_report(\n",
    "                    test_model_output['labels'],\n",
    "                    test_model_output['preds'],\n",
    "                    target_names=['HC', 'PD'],\n",
    "                    output_dict=True,\n",
    "                )\n",
    "                \n",
    "                test_fold_reports[run_dir].append({\n",
    "                    'fold': fold_dir,\n",
    "                    'report': test_fold_report,\n",
    "                    'preds': test_model_output['preds'],\n",
    "                    'labels': test_model_output['labels'],\n",
    "                    'probs': test_model_output['probs'],  \n",
    "                    'auc': test_auc  \n",
    "                })\n",
    "    \n",
    "    return val_fold_reports, test_fold_reports\n",
    "\n",
    "def compute_fold_stats(fold_reports):\n",
    "\n",
    "    fold_stats = {}\n",
    "    \n",
    "    for run_dir, fold_results in fold_reports.items():\n",
    "        accuracies = [fold_result['report']['accuracy'] for fold_result in fold_results]\n",
    "        f1_scores = [fold_result['report']['PD']['f1-score'] for fold_result in fold_results]\n",
    "        precisions = [fold_result['report']['PD']['precision'] for fold_result in fold_results]\n",
    "        recalls = [fold_result['report']['PD']['recall'] for fold_result in fold_results]\n",
    "        \n",
    "        sensitivities = [fold_result['report']['PD']['recall'] for fold_result in fold_results]  \n",
    "        specificities = [fold_result['report']['HC']['recall'] for fold_result in fold_results] \n",
    "        \n",
    "        aucs = [fold_result['auc'] for fold_result in fold_results]\n",
    "        \n",
    "        fold_stats[run_dir] = {\n",
    "            'accuracy': {\n",
    "                'mean': np.mean(accuracies),\n",
    "                'std': np.std(accuracies)\n",
    "            },\n",
    "            'f1': {\n",
    "                'mean': np.mean(f1_scores),\n",
    "                'std': np.std(f1_scores)\n",
    "            },\n",
    "            'precision': {\n",
    "                'mean': np.mean(precisions),\n",
    "                'std': np.std(precisions)\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': np.mean(recalls),\n",
    "                'std': np.std(recalls)\n",
    "            },\n",
    "            'sensitivity': {\n",
    "                'mean': np.mean(sensitivities),\n",
    "                'std': np.std(sensitivities)\n",
    "            },\n",
    "            'specificity': {\n",
    "                'mean': np.mean(specificities),\n",
    "                'std': np.std(specificities)\n",
    "            },\n",
    "            'auc': {\n",
    "                'mean': np.mean(aucs),\n",
    "                'std': np.std(aucs)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return fold_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a8258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical comparison between: \n",
      "- Mallsent and \n",
      "- Mallsent\n",
      "\n",
      "Wilcoxon Signed-Rank Test Results:\n",
      "ACCURACY: 0.7943 > 0.7601, p=0.0039 (* significant *)\n",
      "F1: 0.7924 > 0.7588, p=0.0039 (* significant *)\n",
      "PRECISION: 0.8009 > 0.7678, p=0.0098 (* significant *)\n",
      "SENSITIVITY: 0.7882 > 0.7562, p=0.0039 (* significant *)\n",
      "SPECIFICITY: 0.8005 > 0.7641, p=0.0195 (* significant *)\n",
      "AUC: 0.8445 > 0.8226, p=0.0195 (* significant *)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_two_exps_with_wilcoxon(exps_dir1, exps_dir2):\n",
    "    \"\"\"\n",
    "    Compare results from two experiment directories using Wilcoxon signed-rank test\n",
    "    \n",
    "    Args:\n",
    "        exps_dir1: First experiment directory path\n",
    "        exps_dir2: Second experiment directory path\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with p-values for each metric\n",
    "    \"\"\"\n",
    "    # Get reports for both experiment directories\n",
    "    _, test_fold_reports1 = get_report_perfold(exps_dir1)\n",
    "    _, test_fold_reports2 = get_report_perfold(exps_dir2)\n",
    "    \n",
    "    # Store metrics per fold for both experiments\n",
    "    metrics_by_fold1 = defaultdict(lambda: defaultdict(list))\n",
    "    metrics_by_fold2 = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Process first experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports1.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold1[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold1[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold1[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold1[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold1[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    # Process second experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports2.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold2[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold2[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold2[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold2[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold2[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold2[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold2[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    # Calculate mean values for each fold\n",
    "    metrics_to_test = ['accuracy', 'f1', 'precision', 'sensitivity', 'specificity', 'auc']\n",
    "    fold_means1 = defaultdict(list)\n",
    "    fold_means2 = defaultdict(list)\n",
    "    \n",
    "    # Make sure we only use folds that exist in both experiments\n",
    "    common_folds = set(metrics_by_fold1.keys()) & set(metrics_by_fold2.keys())\n",
    "    \n",
    "    # Calculate means for each fold\n",
    "    for fold in common_folds:\n",
    "        for metric in metrics_to_test:\n",
    "            if metrics_by_fold1[fold][metric] and metrics_by_fold2[fold][metric]:\n",
    "                fold_means1[metric].append(np.mean(metrics_by_fold1[fold][metric]))\n",
    "                fold_means2[metric].append(np.mean(metrics_by_fold2[fold][metric]))\n",
    "    \n",
    "    # print(f\"Number of common folds: {len(common_folds)}\")\n",
    "    # print(f\"Metrics to test: {metrics_to_test}\")\n",
    "    # print(f\"Fold means for experiment 1: {fold_means1}\")\n",
    "    # print(f\"Fold means for experiment 2: {fold_means2}\")\n",
    "    \n",
    "    # Run Wilcoxon signed-rank test for each metric\n",
    "    results = {}\n",
    "    for metric in metrics_to_test:\n",
    "        if len(fold_means1[metric]) >= 5:  # Need at least 5 pairs for reliable results\n",
    "            statistic, p_value = stats.wilcoxon(fold_means1[metric], fold_means2[metric])\n",
    "            results[metric] = {\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'means1': np.mean(fold_means1[metric]),\n",
    "                'means2': np.mean(fold_means2[metric]),\n",
    "                'difference': np.mean(fold_means1[metric]) - np.mean(fold_means2[metric])\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "exps_dir1 = \"/home/yzhong/gits/interpretable-pd/exps/gita_splits2/M3/combined_set/\"\n",
    "exps_dir2 = \"/home/yzhong/gits/interpretable-pd/exps/gita_splits2/M4/combined_set/\"\n",
    "\n",
    "\n",
    "results = compare_two_exps_with_wilcoxon(exps_dir1, exps_dir2)\n",
    "\n",
    "# Print results in a nice format\n",
    "print(f\"Statistical comparison between: \\n- {exps_dir1.strip('/').split('/')[-2]} and \\n- {exps_dir2.strip('/').split('/')[-2]}\\n\")\n",
    "print(\"Wilcoxon Signed-Rank Test Results:\")\n",
    "for metric, result in results.items():\n",
    "    significance = \"* significant *\" if result['significant'] else \"not significant\"\n",
    "    direction = \">\" if result['difference'] > 0 else \"<\"\n",
    "    print(f\"{metric.upper()}: {result['means1']:.4f} {direction} {result['means2']:.4f}, p={result['p_value']:.4f} ({significance})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
