{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40b287b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "def compute_average_report_across_runs(reports):\n",
    "    overall_report = {\n",
    "        'HC': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'PD': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'accuracy': [],\n",
    "        'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "    }\n",
    "\n",
    "    # Spoiler: It's gonna be inefficient :)\n",
    "\n",
    "    for report in reports:\n",
    "        for key in report.keys():\n",
    "            if key == 'accuracy':\n",
    "                overall_report[key].append(report[key])\n",
    "            else:\n",
    "                for key2 in report[key].keys():\n",
    "                    overall_report[key][key2].append(report[key][key2])\n",
    "\n",
    "    for key in overall_report.keys():\n",
    "            if key == 'accuracy':\n",
    "                overall_report[key] = f'{round(np.array(overall_report[key]).mean(), 4)}±{round(np.array(overall_report[key]).std(), 4)}'\n",
    "            else:\n",
    "                for key2 in report[key].keys():\n",
    "                    overall_report[key][key2] = f'{round(np.array(overall_report[key][key2]).mean(), 4)}±{round(np.array(overall_report[key][key2]).std(), 4)}'\n",
    "\n",
    "    # -- just for a more clean output\n",
    "    overall_report = pd.DataFrame.from_dict(overall_report).T\n",
    "    overall_report.iloc[2,0] = ''\n",
    "    overall_report.iloc[2,1] = ''\n",
    "    overall_report.iloc[2,3] = overall_report.iloc[3,3]\n",
    "\n",
    "    return overall_report\n",
    "\n",
    "def get_reports(exps_dir):\n",
    "    val_reports = []\n",
    "    test_reports = []\n",
    "    run_dirs = os.listdir(exps_dir)\n",
    "    for run_dir in run_dirs:\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "\n",
    "        run_dir_path = os.path.join(exps_dir, run_dir)\n",
    "        fold_dirs = os.listdir(run_dir_path)\n",
    "        \n",
    "        for fold_dir in fold_dirs:\n",
    "            model_output_dir = os.path.join(run_dir_path, fold_dir, 'model_output')\n",
    "\n",
    "            # -- validation set\n",
    "            val_report_path = os.path.join(os.path.join(model_output_dir, 'validation_classification.pkl'))\n",
    "            with open(val_report_path, 'rb') as f:\n",
    "                val_model_output = pickle.load(f)\n",
    "\n",
    "            val_preds += val_model_output['preds']\n",
    "            val_labels += val_model_output['labels']\n",
    "\n",
    "            # -- test set\n",
    "            test_report_path = os.path.join(os.path.join(model_output_dir, 'test_classification.pkl'))\n",
    "            with open(test_report_path, 'rb') as f:\n",
    "                test_model_output = pickle.load(f)\n",
    "\n",
    "            test_preds += test_model_output['preds']\n",
    "            test_labels += test_model_output['labels']\n",
    "\n",
    "        # -- computing reports\n",
    "        val_reports.append(\n",
    "            classification_report(\n",
    "                val_labels,\n",
    "                val_preds,\n",
    "                target_names=['HC', 'PD'],\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        test_reports.append(\n",
    "            classification_report(\n",
    "                test_labels,\n",
    "                test_preds,\n",
    "                target_names=['HC', 'PD'],\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return val_reports, test_reports\n",
    "\n",
    "\n",
    "def get_report_perfold(exps_dir):\n",
    "    \"\"\"\n",
    "    获取每个run的每个fold的报告\n",
    "    \n",
    "    Args:\n",
    "        exps_dir: 存储实验结果的目录路径\n",
    "        \n",
    "    Returns:\n",
    "        val_fold_reports: 字典，键为run_dir，值为该run下每个fold的验证集报告列表\n",
    "        test_fold_reports: 字典，键为run_dir，值为该run下每个fold的测试集报告列表\n",
    "    \"\"\"\n",
    "    val_fold_reports = {}\n",
    "    test_fold_reports = {}\n",
    "    run_dirs = sorted(os.listdir(exps_dir))\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        run_dir_path = os.path.join(exps_dir, run_dir)\n",
    "        fold_dirs = sorted(os.listdir(run_dir_path))\n",
    "        \n",
    "        val_fold_reports[run_dir] = []\n",
    "        test_fold_reports[run_dir] = []\n",
    "        \n",
    "        for fold_dir in fold_dirs:\n",
    "            model_output_dir = os.path.join(run_dir_path, fold_dir, 'model_output')\n",
    "            \n",
    "            # -- validation set\n",
    "            val_report_path = os.path.join(model_output_dir, 'validation_classification.pkl')\n",
    "            if os.path.exists(val_report_path):\n",
    "                with open(val_report_path, 'rb') as f:\n",
    "                    val_model_output = pickle.load(f)\n",
    "                \n",
    "                # 提取概率值用于AUC计算\n",
    "                val_probs = [preds[1] for preds in val_model_output['probs']]\n",
    "                val_model_output['probs'] = val_probs  # 更新为单一概率值\n",
    "                \n",
    "                # 计算AUC\n",
    "                val_auc = roc_auc_score(val_model_output['labels'], val_probs)\n",
    "                \n",
    "                # 计算单个fold的验证集分类报告\n",
    "                val_fold_report = classification_report(\n",
    "                    val_model_output['labels'],\n",
    "                    val_model_output['preds'],\n",
    "                    target_names=['HC', 'PD'],\n",
    "                    output_dict=True,\n",
    "                )\n",
    "                \n",
    "                val_fold_reports[run_dir].append({\n",
    "                    'fold': fold_dir,\n",
    "                    'report': val_fold_report,\n",
    "                    'preds': val_model_output['preds'],\n",
    "                    'labels': val_model_output['labels'],\n",
    "                    'probs': val_model_output['probs'],  # 添加概率值\n",
    "                    'auc': val_auc  # 添加AUC\n",
    "                })\n",
    "            \n",
    "            # -- test set\n",
    "            test_report_path = os.path.join(model_output_dir, 'test_classification.pkl')\n",
    "            if os.path.exists(test_report_path):\n",
    "                with open(test_report_path, 'rb') as f:\n",
    "                    test_model_output = pickle.load(f)\n",
    "                \n",
    "                # 提取概率值用于AUC计算\n",
    "                test_probs = [preds[1] for preds in test_model_output['probs']]\n",
    "                test_model_output['probs'] = test_probs  # 更新为单一概率值\n",
    "                \n",
    "                # 计算AUC\n",
    "                test_auc = roc_auc_score(test_model_output['labels'], test_probs)\n",
    "                \n",
    "                # 计算单个fold的测试集分类报告\n",
    "                test_fold_report = classification_report(\n",
    "                    test_model_output['labels'],\n",
    "                    test_model_output['preds'],\n",
    "                    target_names=['HC', 'PD'],\n",
    "                    output_dict=True,\n",
    "                )\n",
    "                \n",
    "                test_fold_reports[run_dir].append({\n",
    "                    'fold': fold_dir,\n",
    "                    'report': test_fold_report,\n",
    "                    'preds': test_model_output['preds'],\n",
    "                    'labels': test_model_output['labels'],\n",
    "                    'probs': test_model_output['probs'],  # 添加概率值\n",
    "                    'auc': test_auc  # 添加AUC\n",
    "                })\n",
    "    \n",
    "    return val_fold_reports, test_fold_reports\n",
    "\n",
    "def compute_fold_stats(fold_reports):\n",
    "    \"\"\"\n",
    "    计算每个run中所有fold的性能指标的均值和标准差\n",
    "    \n",
    "    Args:\n",
    "        fold_reports: 字典，键为run_dir，值为该run下每个fold的报告列表\n",
    "        \n",
    "    Returns:\n",
    "        fold_stats: 字典，键为run_dir，值为包含accuracy、f1、precision、recall、sensitivity、specificity、auc的均值和标准差的字典\n",
    "    \"\"\"\n",
    "    fold_stats = {}\n",
    "    \n",
    "    for run_dir, fold_results in fold_reports.items():\n",
    "        accuracies = [fold_result['report']['accuracy'] for fold_result in fold_results]\n",
    "        f1_scores = [fold_result['report']['PD']['f1-score'] for fold_result in fold_results]\n",
    "        precisions = [fold_result['report']['PD']['precision'] for fold_result in fold_results]\n",
    "        recalls = [fold_result['report']['PD']['recall'] for fold_result in fold_results]\n",
    "        \n",
    "        # 计算灵敏度(Sensitivity)和特异度(Specificity)\n",
    "        sensitivities = [fold_result['report']['PD']['recall'] for fold_result in fold_results]  # 灵敏度就是PD的召回率\n",
    "        specificities = [fold_result['report']['HC']['recall'] for fold_result in fold_results]  # 特异度是HC的召回率\n",
    "        \n",
    "        # 添加AUC指标\n",
    "        aucs = [fold_result['auc'] for fold_result in fold_results]\n",
    "        \n",
    "        fold_stats[run_dir] = {\n",
    "            'accuracy': {\n",
    "                'mean': np.mean(accuracies),\n",
    "                'std': np.std(accuracies)\n",
    "            },\n",
    "            'f1': {\n",
    "                'mean': np.mean(f1_scores),\n",
    "                'std': np.std(f1_scores)\n",
    "            },\n",
    "            'precision': {\n",
    "                'mean': np.mean(precisions),\n",
    "                'std': np.std(precisions)\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': np.mean(recalls),\n",
    "                'std': np.std(recalls)\n",
    "            },\n",
    "            'sensitivity': {\n",
    "                'mean': np.mean(sensitivities),\n",
    "                'std': np.std(sensitivities)\n",
    "            },\n",
    "            'specificity': {\n",
    "                'mean': np.mean(specificities),\n",
    "                'std': np.std(specificities)\n",
    "            },\n",
    "            'auc': {\n",
    "                'mean': np.mean(aucs),\n",
    "                'std': np.std(aucs)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return fold_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b87ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Performance Per Fold:\n",
      "\n",
      "Run: seed12\n",
      "  Fold: fold_0, Accuracy: 0.8543, F1 (PD): 0.8543\n",
      "  Fold: fold_1, Accuracy: 0.6700, F1 (PD): 0.6765\n",
      "  Fold: fold_2, Accuracy: 0.7980, F1 (PD): 0.8113\n",
      "  Fold: fold_3, Accuracy: 0.7750, F1 (PD): 0.7847\n",
      "  Fold: fold_4, Accuracy: 0.7850, F1 (PD): 0.7882\n",
      "  Mean Accuracy: 0.7765 ± 0.0599\n",
      "  Mean F1 (PD): 0.7830 ± 0.0588\n",
      "  Mean Precision (PD): 0.7665 ± 0.0823\n",
      "  Mean Recall (PD): 0.8094 ± 0.0855\n",
      "  Mean Sensitivity: 0.8094 ± 0.0855\n",
      "  Mean Specificity: 0.7510 ± 0.0904\n",
      "\n",
      "=== Average Performance Across All Test Runs ===\n",
      "Mean Accuracy across runs: 0.7718 ± 0.0077\n",
      "Mean F1 (PD) across runs: 0.7765 ± 0.0097\n",
      "Mean Precision (PD) across runs: 0.7662 ± 0.0086\n",
      "Mean Recall (PD) across runs: 0.7988 ± 0.0148\n",
      "Mean Sensitivity across runs: 0.7988 ± 0.0148\n",
      "Mean Specificity across runs: 0.7531 ± 0.0109\n",
      "Mean AUC across runs: 0.8582 ± 0.0056\n",
      "\n",
      "=== Compact Statistics ===\n",
      "Accuracy & F1 (PD) & Precision (PD) & Recall (PD) & AUC & Sensitivity & Specificity\n",
      "0.7718 ± 0.0544 & 0.7765 ± 0.0553 & 0.7662 ± 0.0779 & 0.7988 ± 0.0924 & 0.8582 ± 0.0614 & 0.7988 ± 0.0924 & 0.7531 ± 0.0879\n",
      "\n",
      "Run: seed21\n",
      "  Fold: fold_0, Accuracy: 0.8040, F1 (PD): 0.8060\n",
      "  Fold: fold_1, Accuracy: 0.6900, F1 (PD): 0.6931\n",
      "  Fold: fold_2, Accuracy: 0.8131, F1 (PD): 0.8246\n",
      "  Fold: fold_3, Accuracy: 0.7700, F1 (PD): 0.7788\n",
      "  Fold: fold_4, Accuracy: 0.7750, F1 (PD): 0.7716\n",
      "  Mean Accuracy: 0.7704 ± 0.0435\n",
      "  Mean F1 (PD): 0.7748 ± 0.0451\n",
      "  Mean Precision (PD): 0.7650 ± 0.0716\n",
      "  Mean Recall (PD): 0.7959 ± 0.0873\n",
      "  Mean Sensitivity: 0.7959 ± 0.0873\n",
      "  Mean Specificity: 0.7534 ± 0.0787\n",
      "\n",
      "=== Average Performance Across All Test Runs ===\n",
      "Mean Accuracy across runs: 0.7718 ± 0.0077\n",
      "Mean F1 (PD) across runs: 0.7765 ± 0.0097\n",
      "Mean Precision (PD) across runs: 0.7662 ± 0.0086\n",
      "Mean Recall (PD) across runs: 0.7988 ± 0.0148\n",
      "Mean Sensitivity across runs: 0.7988 ± 0.0148\n",
      "Mean Specificity across runs: 0.7531 ± 0.0109\n",
      "Mean AUC across runs: 0.8582 ± 0.0056\n",
      "\n",
      "=== Compact Statistics ===\n",
      "Accuracy & F1 (PD) & Precision (PD) & Recall (PD) & AUC & Sensitivity & Specificity\n",
      "0.7718 ± 0.0544 & 0.7765 ± 0.0553 & 0.7662 ± 0.0779 & 0.7988 ± 0.0924 & 0.8582 ± 0.0614 & 0.7988 ± 0.0924 & 0.7531 ± 0.0879\n",
      "\n",
      "Run: seed33\n",
      "  Fold: fold_0, Accuracy: 0.8291, F1 (PD): 0.8300\n",
      "  Fold: fold_1, Accuracy: 0.6700, F1 (PD): 0.6887\n",
      "  Fold: fold_2, Accuracy: 0.7879, F1 (PD): 0.8037\n",
      "  Fold: fold_3, Accuracy: 0.8050, F1 (PD): 0.8079\n",
      "  Fold: fold_4, Accuracy: 0.7900, F1 (PD): 0.7981\n",
      "  Mean Accuracy: 0.7764 ± 0.0552\n",
      "  Mean F1 (PD): 0.7857 ± 0.0497\n",
      "  Mean Precision (PD): 0.7604 ± 0.0714\n",
      "  Mean Recall (PD): 0.8189 ± 0.0680\n",
      "  Mean Sensitivity: 0.8189 ± 0.0680\n",
      "  Mean Specificity: 0.7394 ± 0.0836\n",
      "\n",
      "=== Average Performance Across All Test Runs ===\n",
      "Mean Accuracy across runs: 0.7718 ± 0.0077\n",
      "Mean F1 (PD) across runs: 0.7765 ± 0.0097\n",
      "Mean Precision (PD) across runs: 0.7662 ± 0.0086\n",
      "Mean Recall (PD) across runs: 0.7988 ± 0.0148\n",
      "Mean Sensitivity across runs: 0.7988 ± 0.0148\n",
      "Mean Specificity across runs: 0.7531 ± 0.0109\n",
      "Mean AUC across runs: 0.8582 ± 0.0056\n",
      "\n",
      "=== Compact Statistics ===\n",
      "Accuracy & F1 (PD) & Precision (PD) & Recall (PD) & AUC & Sensitivity & Specificity\n",
      "0.7718 ± 0.0544 & 0.7765 ± 0.0553 & 0.7662 ± 0.0779 & 0.7988 ± 0.0924 & 0.8582 ± 0.0614 & 0.7988 ± 0.0924 & 0.7531 ± 0.0879\n",
      "\n",
      "Run: seed42\n",
      "  Fold: fold_0, Accuracy: 0.8392, F1 (PD): 0.8416\n",
      "  Fold: fold_1, Accuracy: 0.6900, F1 (PD): 0.6869\n",
      "  Fold: fold_2, Accuracy: 0.7879, F1 (PD): 0.8056\n",
      "  Fold: fold_3, Accuracy: 0.7600, F1 (PD): 0.7670\n",
      "  Fold: fold_4, Accuracy: 0.7100, F1 (PD): 0.6915\n",
      "  Mean Accuracy: 0.7574 ± 0.0537\n",
      "  Mean F1 (PD): 0.7585 ± 0.0613\n",
      "  Mean Precision (PD): 0.7570 ± 0.0657\n",
      "  Mean Recall (PD): 0.7755 ± 0.1182\n",
      "  Mean Sensitivity: 0.7755 ± 0.1182\n",
      "  Mean Specificity: 0.7489 ± 0.0775\n",
      "\n",
      "=== Average Performance Across All Test Runs ===\n",
      "Mean Accuracy across runs: 0.7718 ± 0.0077\n",
      "Mean F1 (PD) across runs: 0.7765 ± 0.0097\n",
      "Mean Precision (PD) across runs: 0.7662 ± 0.0086\n",
      "Mean Recall (PD) across runs: 0.7988 ± 0.0148\n",
      "Mean Sensitivity across runs: 0.7988 ± 0.0148\n",
      "Mean Specificity across runs: 0.7531 ± 0.0109\n",
      "Mean AUC across runs: 0.8582 ± 0.0056\n",
      "\n",
      "=== Compact Statistics ===\n",
      "Accuracy & F1 (PD) & Precision (PD) & Recall (PD) & AUC & Sensitivity & Specificity\n",
      "0.7718 ± 0.0544 & 0.7765 ± 0.0553 & 0.7662 ± 0.0779 & 0.7988 ± 0.0924 & 0.8582 ± 0.0614 & 0.7988 ± 0.0924 & 0.7531 ± 0.0879\n",
      "\n",
      "Run: seed52\n",
      "  Fold: fold_0, Accuracy: 0.8342, F1 (PD): 0.8272\n",
      "  Fold: fold_1, Accuracy: 0.6650, F1 (PD): 0.6599\n",
      "  Fold: fold_2, Accuracy: 0.8081, F1 (PD): 0.8208\n",
      "  Fold: fold_3, Accuracy: 0.7750, F1 (PD): 0.7887\n",
      "  Fold: fold_4, Accuracy: 0.8100, F1 (PD): 0.8061\n",
      "  Mean Accuracy: 0.7785 ± 0.0598\n",
      "  Mean F1 (PD): 0.7805 ± 0.0618\n",
      "  Mean Precision (PD): 0.7819 ± 0.0986\n",
      "  Mean Recall (PD): 0.7941 ± 0.1031\n",
      "  Mean Sensitivity: 0.7941 ± 0.1031\n",
      "  Mean Specificity: 0.7728 ± 0.1092\n",
      "\n",
      "=== Average Performance Across All Test Runs ===\n",
      "Mean Accuracy across runs: 0.7718 ± 0.0077\n",
      "Mean F1 (PD) across runs: 0.7765 ± 0.0097\n",
      "Mean Precision (PD) across runs: 0.7662 ± 0.0086\n",
      "Mean Recall (PD) across runs: 0.7988 ± 0.0148\n",
      "Mean Sensitivity across runs: 0.7988 ± 0.0148\n",
      "Mean Specificity across runs: 0.7531 ± 0.0109\n",
      "Mean AUC across runs: 0.8582 ± 0.0056\n",
      "\n",
      "=== Compact Statistics ===\n",
      "Accuracy & F1 (PD) & Precision (PD) & Recall (PD) & AUC & Sensitivity & Specificity\n",
      "0.7718 ± 0.0544 & 0.7765 ± 0.0553 & 0.7662 ± 0.0779 & 0.7988 ± 0.0924 & 0.8582 ± 0.0614 & 0.7988 ± 0.0924 & 0.7531 ± 0.0879\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exps_dir = \"/home/yzhong/gits/interpretable-pd/exps/gita-splitmono//cross_full/MONOLOGUE/\"\n",
    "# 获取每个fold的报告\n",
    "val_fold_reports, test_fold_reports = get_report_perfold(exps_dir)\n",
    "\n",
    "# 计算每个run的统计数据\n",
    "# val_fold_stats = compute_fold_stats(val_fold_reports)\n",
    "test_fold_stats = compute_fold_stats(test_fold_reports)\n",
    "\n",
    "# 打印每个run每个fold的结果\n",
    "print(\"\\nTest Performance Per Fold:\")\n",
    "for run_dir, fold_results in test_fold_reports.items():\n",
    "    print(f\"\\nRun: {run_dir}\")\n",
    "    for fold_result in fold_results:\n",
    "        fold = fold_result['fold']\n",
    "        report = fold_result['report']\n",
    "        print(f\"  Fold: {fold}, Accuracy: {report['accuracy']:.4f}, F1 (PD): {report['PD']['f1-score']:.4f}\")\n",
    "    \n",
    "    # 打印该run的所有fold的均值和标准差\n",
    "    stats = test_fold_stats[run_dir]\n",
    "    # 打印该run的所有fold的均值和标准差\n",
    "    print(f\"  Mean Accuracy: {stats['accuracy']['mean']:.4f} ± {stats['accuracy']['std']:.4f}\")\n",
    "    print(f\"  Mean F1 (PD): {stats['f1']['mean']:.4f} ± {stats['f1']['std']:.4f}\")\n",
    "    print(f\"  Mean Precision (PD): {stats['precision']['mean']:.4f} ± {stats['precision']['std']:.4f}\")\n",
    "    print(f\"  Mean Recall (PD): {stats['recall']['mean']:.4f} ± {stats['recall']['std']:.4f}\")\n",
    "    print(f\"  Mean Sensitivity: {stats['sensitivity']['mean']:.4f} ± {stats['sensitivity']['std']:.4f}\")\n",
    "    print(f\"  Mean Specificity: {stats['specificity']['mean']:.4f} ± {stats['specificity']['std']:.4f}\")\n",
    "# 计算所有runs的平均性能指标和标准差\n",
    "    print(\"\\n=== Average Performance Across All Test Runs ===\")\n",
    "    all_run_accuracies = [stats['accuracy']['mean'] for stats in test_fold_stats.values()]\n",
    "    all_run_f1s = [stats['f1']['mean'] for stats in test_fold_stats.values()]\n",
    "    all_run_precisions = [stats['precision']['mean'] for stats in test_fold_stats.values()]\n",
    "    all_run_recalls = [stats['recall']['mean'] for stats in test_fold_stats.values()]\n",
    "    all_run_sensitivities = [stats['sensitivity']['mean'] for stats in test_fold_stats.values()]\n",
    "    all_run_specificities = [stats['specificity']['mean'] for stats in test_fold_stats.values()]\n",
    "\n",
    "    # 计算所有runs的标准差的平均值\n",
    "    all_run_accuracies_stds = [stats['accuracy']['std'] for stats in test_fold_stats.values()]\n",
    "    all_run_f1s_stds = [stats['f1']['std'] for stats in test_fold_stats.values()]\n",
    "    all_run_precisions_stds = [stats['precision']['std'] for stats in test_fold_stats.values()]\n",
    "    all_run_recalls_stds = [stats['recall']['std'] for stats in test_fold_stats.values()]\n",
    "    all_run_sensitivities_stds = [stats['sensitivity']['std'] for stats in test_fold_stats.values()]\n",
    "    all_run_specificities_stds = [stats['specificity']['std'] for stats in test_fold_stats.values()]\n",
    "    # 汇总统计部分也需要加入AUC\n",
    "    all_run_aucs = [stats['auc']['mean'] for stats in test_fold_stats.values()]\n",
    "    all_run_aucs_stds = [stats['auc']['std'] for stats in test_fold_stats.values()]\n",
    "\n",
    "    # 打印所有runs的平均性能\n",
    "    print(f\"Mean Accuracy across runs: {np.mean(all_run_accuracies):.4f} ± {np.std(all_run_accuracies):.4f}\")\n",
    "    print(f\"Mean F1 (PD) across runs: {np.mean(all_run_f1s):.4f} ± {np.std(all_run_f1s):.4f}\")\n",
    "    print(f\"Mean Precision (PD) across runs: {np.mean(all_run_precisions):.4f} ± {np.std(all_run_precisions):.4f}\")\n",
    "    print(f\"Mean Recall (PD) across runs: {np.mean(all_run_recalls):.4f} ± {np.std(all_run_recalls):.4f}\")\n",
    "    print(f\"Mean Sensitivity across runs: {np.mean(all_run_sensitivities):.4f} ± {np.std(all_run_sensitivities):.4f}\")\n",
    "    print(f\"Mean Specificity across runs: {np.mean(all_run_specificities):.4f} ± {np.std(all_run_specificities):.4f}\")\n",
    "    print(f\"Mean AUC across runs: {np.mean(all_run_aucs):.4f} ± {np.std(all_run_aucs):.4f}\")\n",
    "\n",
    "\n",
    "    print(\"\\n=== Compact Statistics ===\")\n",
    "    print(\"Accuracy & F1 (PD) & Precision (PD) & Recall (PD) & AUC & Sensitivity & Specificity\")\n",
    "    print(f\"{np.mean(all_run_accuracies):.4f} ± {np.mean(all_run_accuracies_stds):.4f} & {np.mean(all_run_f1s):.4f} ± {np.mean(all_run_f1s_stds):.4f} & {np.mean(all_run_precisions):.4f} ± {np.mean(all_run_precisions_stds):.4f} & {np.mean(all_run_recalls):.4f} ± {np.mean(all_run_recalls_stds):.4f} & {np.mean(all_run_aucs):.4f} ± {np.mean(all_run_aucs_stds):.4f} & {np.mean(all_run_sensitivities):.4f} ± {np.mean(all_run_sensitivities_stds):.4f} & {np.mean(all_run_specificities):.4f} ± {np.mean(all_run_specificities_stds):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac0a8258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical comparison between: \n",
      "- Mallsent and \n",
      "- Mallsent\n",
      "\n",
      "Wilcoxon Signed-Rank Test Results:\n",
      "ACCURACY: 0.7787 > 0.7601, p=0.0273 (* significant *)\n",
      "F1: 0.7748 > 0.7588, p=0.0137 (* significant *)\n",
      "PRECISION: 0.7923 > 0.7678, p=0.1602 (not significant)\n",
      "SENSITIVITY: 0.7652 > 0.7562, p=0.2754 (not significant)\n",
      "SPECIFICITY: 0.7924 > 0.7641, p=0.3223 (not significant)\n",
      "AUC: 0.8301 > 0.8226, p=0.2754 (not significant)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_two_exps_with_wilcoxon(exps_dir1, exps_dir2):\n",
    "    \"\"\"\n",
    "    Compare results from two experiment directories using Wilcoxon signed-rank test\n",
    "    \n",
    "    Args:\n",
    "        exps_dir1: First experiment directory path\n",
    "        exps_dir2: Second experiment directory path\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with p-values for each metric\n",
    "    \"\"\"\n",
    "    # Get reports for both experiment directories\n",
    "    _, test_fold_reports1 = get_report_perfold(exps_dir1)\n",
    "    _, test_fold_reports2 = get_report_perfold(exps_dir2)\n",
    "    \n",
    "    # Store metrics per fold for both experiments\n",
    "    metrics_by_fold1 = defaultdict(lambda: defaultdict(list))\n",
    "    metrics_by_fold2 = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Process first experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports1.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold1[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold1[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold1[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold1[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold1[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    # Process second experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports2.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold2[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold2[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold2[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold2[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold2[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold2[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold2[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    # Calculate mean values for each fold\n",
    "    metrics_to_test = ['accuracy', 'f1', 'precision', 'sensitivity', 'specificity', 'auc']\n",
    "    fold_means1 = defaultdict(list)\n",
    "    fold_means2 = defaultdict(list)\n",
    "    \n",
    "    # Make sure we only use folds that exist in both experiments\n",
    "    common_folds = set(metrics_by_fold1.keys()) & set(metrics_by_fold2.keys())\n",
    "    \n",
    "    # Calculate means for each fold\n",
    "    for fold in common_folds:\n",
    "        for metric in metrics_to_test:\n",
    "            if metrics_by_fold1[fold][metric] and metrics_by_fold2[fold][metric]:\n",
    "                fold_means1[metric].append(np.mean(metrics_by_fold1[fold][metric]))\n",
    "                fold_means2[metric].append(np.mean(metrics_by_fold2[fold][metric]))\n",
    "    \n",
    "    # print(f\"Number of common folds: {len(common_folds)}\")\n",
    "    # print(f\"Metrics to test: {metrics_to_test}\")\n",
    "    # print(f\"Fold means for experiment 1: {fold_means1}\")\n",
    "    # print(f\"Fold means for experiment 2: {fold_means2}\")\n",
    "    \n",
    "    # Run Wilcoxon signed-rank test for each metric\n",
    "    results = {}\n",
    "    for metric in metrics_to_test:\n",
    "        if len(fold_means1[metric]) >= 5:  # Need at least 5 pairs for reliable results\n",
    "            statistic, p_value = stats.wilcoxon(fold_means1[metric], fold_means2[metric])\n",
    "            results[metric] = {\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'means1': np.mean(fold_means1[metric]),\n",
    "                'means2': np.mean(fold_means2[metric]),\n",
    "                'difference': np.mean(fold_means1[metric]) - np.mean(fold_means2[metric])\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "exps_dir1 = \"/home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full/allsent/\"\n",
    "exps_dir2 = \"/home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full_fix_value/allsent/\"\n",
    "\n",
    "\n",
    "results = compare_two_exps_with_wilcoxon(exps_dir1, exps_dir2)\n",
    "\n",
    "# Print results in a nice format\n",
    "print(f\"Statistical comparison between: \\n- M{exps_dir1.strip('/').split('/')[-1]} and \\n- M{exps_dir2.strip('/').split('/')[-1]}\\n\")\n",
    "print(\"Wilcoxon Signed-Rank Test Results:\")\n",
    "for metric, result in results.items():\n",
    "    significance = \"* significant *\" if result['significant'] else \"not significant\"\n",
    "    direction = \">\" if result['difference'] > 0 else \"<\"\n",
    "    print(f\"{metric.upper()}: {result['means1']:.4f} {direction} {result['means2']:.4f}, p={result['p_value']:.4f} ({significance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b61138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_avg_F1_for_allfolds(exps_dir1):\n",
    "    \"\"\"\n",
    "    Compare results from two experiment directories using Wilcoxon signed-rank test\n",
    "    \n",
    "    Args:\n",
    "        exps_dir1: First experiment directory path\n",
    "        exps_dir2: Second experiment directory path\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with p-values for each metric\n",
    "    \"\"\"\n",
    "    \n",
    "    # print(f\"Processing experiment directory: {exps_dir1}\")\n",
    "    # Get reports for both experiment directories\n",
    "    _, test_fold_reports1 = get_report_perfold(exps_dir1)\n",
    "    \n",
    "    # Store metrics per fold for both experiments\n",
    "    metrics_by_fold1 = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Process first experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports1.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold1[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold1[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold1[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold1[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold1[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    \n",
    "    # Calculate mean values for each fold\n",
    "    metrics_to_test = ['accuracy', 'f1', 'precision', 'sensitivity', 'specificity', 'auc']\n",
    "    fold_means1 = defaultdict(list)\n",
    "    \n",
    "    # Make sure we only use folds that exist in both experiments\n",
    "    common_folds = set(metrics_by_fold1.keys()) \n",
    "    fold_means1_f1 = []\n",
    "    # Calculate means for each fold\n",
    "    for i, fold in enumerate(common_folds):\n",
    "\n",
    "        if metrics_by_fold1[fold]['f1'] :\n",
    "            fold_means1_f1.append([i+1, np.mean(metrics_by_fold1[fold]['f1'])])\n",
    "            \n",
    "    return fold_means1_f1\n",
    "\n",
    "exp1 = \"cross_full\"\n",
    "exp2 = \"cross_full_fix\"\n",
    "exp3 = \"cross_full_fix_value\"\n",
    "exp4 = \"cross_token_oldcate\"\n",
    "exps = [exp1, exp2, exp3, exp4]\n",
    "\n",
    "def get_report_perexp(exp):\n",
    "    \n",
    "    dir1 = \"/home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/\"\n",
    "    dir2 = \"/home/yzhong/gits/interpretable-pd/exps/gita/\"\n",
    "    dir3 = \"/home/yzhong/gits/interpretable-pd/exps/gita-splitmono/\"\n",
    "    pertask2f1 = defaultdict(list)\n",
    "\n",
    "    exps_dir1 = os.path.join(dir1, exp, 'allsent') \n",
    "    fold_means_allsent = get_avg_F1_for_allfolds(exps_dir1)\n",
    "    pertask2f1['ALLSENT'] = fold_means_allsent\n",
    "    \n",
    "    exps_dir3 = os.path.join(dir3, exp, 'MONOLOGUE')\n",
    "    fold_means_splitmono = get_avg_F1_for_allfolds(exps_dir3)\n",
    "    pertask2f1['SPLIT-MONO'] = fold_means_splitmono\n",
    "\n",
    "    tasks = [\"DDK\", \"READ\", \"SENTENCES\", \"SUSTAINED-VOWELS\", \"WORDS\"]\n",
    "    for task in tasks:\n",
    "        exp_dir = os.path.join(dir2, exp, task)\n",
    "        if os.path.exists(exp_dir):\n",
    "            fold_means = get_avg_F1_for_allfolds(exp_dir)\n",
    "            pertask2f1[task] = fold_means\n",
    "            \n",
    "    \n",
    "\n",
    "    return pertask2f1\n",
    "\n",
    "\n",
    "pertask2f1_1 = get_report_perexp(exp1)\n",
    "pertask2f1_2 = get_report_perexp(exp2)\n",
    "pertask2f1_3 = get_report_perexp(exp3)\n",
    "pertask2f1_4 = get_report_perexp(exp4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a25e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame:\n",
      "                Task  Fold  F1_Score ModelID\n",
      "0            ALLSENT     1    0.7673      M1\n",
      "1            ALLSENT     2    0.8066      M1\n",
      "2            ALLSENT     3    0.8320      M1\n",
      "3            ALLSENT     4    0.8796      M1\n",
      "4            ALLSENT     5    0.8073      M1\n",
      "5            ALLSENT     6    0.8621      M1\n",
      "6            ALLSENT     7    0.6553      M1\n",
      "7            ALLSENT     8    0.7682      M1\n",
      "8            ALLSENT     9    0.6336      M1\n",
      "9            ALLSENT    10    0.7358      M1\n",
      "10               DDK    21    0.6996      M1\n",
      "11               DDK    22    0.8127      M1\n",
      "12               DDK    23    0.7864      M1\n",
      "13               DDK    24    0.8372      M1\n",
      "14               DDK    25    0.7137      M1\n",
      "15              READ    31    0.7487      M1\n",
      "16              READ    32    0.8189      M1\n",
      "17              READ    33    0.5260      M1\n",
      "18              READ    34    0.8692      M1\n",
      "19              READ    35    0.8405      M1\n",
      "20         SENTENCES    41    0.6243      M1\n",
      "21         SENTENCES    42    0.8131      M1\n",
      "22         SENTENCES    43    0.7844      M1\n",
      "23         SENTENCES    44    0.7946      M1\n",
      "24         SENTENCES    45    0.8663      M1\n",
      "25        SPLIT-MONO    11    0.6810      M1\n",
      "26        SPLIT-MONO    12    0.8132      M1\n",
      "27        SPLIT-MONO    13    0.7711      M1\n",
      "28        SPLIT-MONO    14    0.7854      M1\n",
      "29        SPLIT-MONO    15    0.8318      M1\n",
      "30  SUSTAINED-VOWELS    51    0.5679      M1\n",
      "31  SUSTAINED-VOWELS    52    0.7119      M1\n",
      "32  SUSTAINED-VOWELS    53    0.5783      M1\n",
      "33  SUSTAINED-VOWELS    54    0.7092      M1\n",
      "34  SUSTAINED-VOWELS    55    0.7178      M1\n",
      "35             WORDS    61    0.6501      M1\n",
      "36             WORDS    62    0.7749      M1\n",
      "37             WORDS    63    0.7251      M1\n",
      "38             WORDS    64    0.7712      M1\n",
      "39             WORDS    65    0.7350      M1\n",
      "40           ALLSENT     1    0.7842      M2\n",
      "41           ALLSENT     2    0.8143      M2\n",
      "42           ALLSENT     3    0.8335      M2\n",
      "43           ALLSENT     4    0.9203      M2\n",
      "44           ALLSENT     5    0.7873      M2\n",
      "45           ALLSENT     6    0.8870      M2\n",
      "46           ALLSENT     7    0.6675      M2\n",
      "47           ALLSENT     8    0.8034      M2\n",
      "48           ALLSENT     9    0.6530      M2\n",
      "49           ALLSENT    10    0.7738      M2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_nested_dict_to_dataframe(data_dict, modelid=\"M1\"):\n",
    "    \"\"\"\n",
    "    Convert a nested defaultdict to a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        data_dict: Nested defaultdict with task names as keys and lists of [fold_number, f1_score] as values\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with tasks, fold numbers, and F1 scores in long format\n",
    "    \"\"\"\n",
    "    # Create lists to store the data in long format\n",
    "    tasks = []\n",
    "    fold_numbers = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    # Process each task\n",
    "    taskidx = 0\n",
    "    for task, fold_scores in data_dict.items():\n",
    "        for fold_num, score in fold_scores:\n",
    "            tasks.append(task)\n",
    "            fold_numbers.append(int(fold_num) + taskidx * 10)  # Adjust fold number to be unique across tasks\n",
    "            f1_scores.append(score)\n",
    "        taskidx += 1  # Increment task index to adjust fold numbers\n",
    "    \n",
    "    # Create DataFrame in long format\n",
    "    df = pd.DataFrame({\n",
    "        'Task': tasks,\n",
    "        'Fold': fold_numbers,\n",
    "        'F1_Score': f1_scores,\n",
    "        'ModelID': modelid\n",
    "    })\n",
    "    \n",
    "    # Sort by task and fold\n",
    "    df = df.sort_values(by=['Task', 'Fold']).reset_index(drop=True)\n",
    "    \n",
    "    # Add a sequential ID column\n",
    "    # df['ID'] = range(1, len(df) + 1)\n",
    "    \n",
    "    # Format the DataFrame for better display\n",
    "    pd.options.display.float_format = '{:.4f}'.format\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "df1 = convert_nested_dict_to_dataframe(pertask2f1_1, modelid=\"M1\")\n",
    "df2 = convert_nested_dict_to_dataframe(pertask2f1_2, modelid=\"M2\")\n",
    "df3 = convert_nested_dict_to_dataframe(pertask2f1_3, modelid=\"M3\")\n",
    "df4 = convert_nested_dict_to_dataframe(pertask2f1_4, modelid=\"M4\")\n",
    "\n",
    "\n",
    "df_all = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Print the combined dataframe\n",
    "print(\"Combined DataFrame:\")\n",
    "print(df_all[:50])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e259cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 162\u001b[0m\n\u001b[1;32m    142\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# def main():\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m#     # Load data from the four models\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m#     df1 = pd.read_csv('/home/yzhong/gits/interpretable-pd/splits/model1_results.csv')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Fit LME model\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m model, result \u001b[38;5;241m=\u001b[39m \u001b[43mfit_lme_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mfit_lme_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Fit the LME model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Using Treatment contrast with M1 as reference level\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m smf\u001b[38;5;241m.\u001b[39mmixedlm(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF1_Score ~ ModelID\u001b[39m\u001b[38;5;124m\"\u001b[39m, df, \n\u001b[1;32m     24\u001b[0m                      groups\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     25\u001b[0m                      re_formula\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m~Fold\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, result\n",
      "File \u001b[0;32m/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2257\u001b[0m, in \u001b[0;36mMixedLM.fit\u001b[0;34m(self, start_params, reml, niter_sa, do_cg, fe_pen, cov_pen, free, full_output, method, **fit_kwargs)\u001b[0m\n\u001b[1;32m   2255\u001b[0m         pcov[np\u001b[38;5;241m.\u001b[39mix_(ii, ii)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(\u001b[38;5;241m-\u001b[39mhess1)\n\u001b[1;32m   2256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2257\u001b[0m     pcov \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(hess_diag \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m   2259\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Hessian matrix at the estimated parameter values \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   2260\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not positive definite.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/storage500/yzhongenv/lib/python3.10/site-packages/numpy/linalg/linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m/data/storage500/yzhongenv/lib/python3.10/site-packages/numpy/linalg/linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def fit_lme_model(df):\n",
    "    \"\"\"\n",
    "    Fit a Linear Mixed Effects model with F1_Score as response variable,\n",
    "    ModelID as fixed effect, and Task and Fold as random effects.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data with columns 'Task', 'Fold', 'F1_Score', and 'ModelID'\n",
    "        \n",
    "    Returns:\n",
    "        fitted model and summary\n",
    "    \"\"\"\n",
    "    # Fit the LME model\n",
    "    # Using Treatment contrast with M1 as reference level\n",
    "    model = smf.mixedlm(\"F1_Score ~ ModelID\", df, \n",
    "                         groups=df[\"Task\"],\n",
    "                         re_formula=\"~Fold\")\n",
    "    \n",
    "    result = model.fit()\n",
    "    return model, result\n",
    "\n",
    "def run_pairwise_comparisons(df):\n",
    "    \"\"\"\n",
    "    Run pairwise comparisons between all model combinations\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with pairwise comparison results\n",
    "    \"\"\"\n",
    "    # Get unique model IDs\n",
    "    models = df['ModelID'].unique()\n",
    "    \n",
    "    # Create empty lists to store comparison results\n",
    "    pair_list = []\n",
    "    diff_list = []\n",
    "    pvalue_list = []\n",
    "    significant_list = []\n",
    "    \n",
    "    # Iterate through all pairs of models\n",
    "    for m1, m2 in combinations(models, 2):\n",
    "        # Subset data for the two models\n",
    "        data_m1 = df[df['ModelID'] == m1]['F1_Score']\n",
    "        data_m2 = df[df['ModelID'] == m2]['F1_Score']\n",
    "        \n",
    "        # Calculate mean difference\n",
    "        mean_diff = data_m1.mean() - data_m2.mean()\n",
    "        \n",
    "        # Fit a model for just these two models\n",
    "        subset_df = df[df['ModelID'].isin([m1, m2])].copy()\n",
    "        subset_df['ModelID'] = pd.Categorical(subset_df['ModelID'], \n",
    "                                             categories=[m1, m2], \n",
    "                                             ordered=False)\n",
    "        \n",
    "        model = smf.mixedlm(\"F1_Score ~ ModelID\", subset_df, \n",
    "                           groups=subset_df[\"Task\"],\n",
    "                           re_formula=\"~Fold\")\n",
    "        result = model.fit()\n",
    "        \n",
    "        # Get p-value for the model comparison\n",
    "        p_value = result.pvalues['ModelID[T.{}]'.format(m2)]\n",
    "        significant = p_value < 0.05\n",
    "        \n",
    "        # Append results\n",
    "        pair_list.append(f\"{m1} vs {m2}\")\n",
    "        diff_list.append(mean_diff)\n",
    "        pvalue_list.append(p_value)\n",
    "        significant_list.append(significant)\n",
    "    \n",
    "    # Create comparison results DataFrame\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Comparison': pair_list,\n",
    "        'Mean_Difference': diff_list,\n",
    "        'P_Value': pvalue_list,\n",
    "        'Significant': significant_list\n",
    "    })\n",
    "    \n",
    "    return comparison_df.sort_values('P_Value')\n",
    "\n",
    "def visualize_results(df, lme_result, comparison_df):\n",
    "    \"\"\"\n",
    "    Create visualizations of the model results\n",
    "    \n",
    "    Args:\n",
    "        df: Original DataFrame\n",
    "        lme_result: Fitted LME model result\n",
    "        comparison_df: DataFrame with pairwise comparison results\n",
    "    \"\"\"\n",
    "    # Set up the figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # 1. Box plot of F1 scores by model\n",
    "    sns.boxplot(x='ModelID', y='F1_Score', data=df, ax=ax1)\n",
    "    ax1.set_title('Distribution of F1 Scores by Model')\n",
    "    ax1.set_xlabel('Model')\n",
    "    ax1.set_ylabel('F1 Score')\n",
    "    \n",
    "    # Add mean F1 score labels\n",
    "    model_means = df.groupby('ModelID')['F1_Score'].mean()\n",
    "    for i, model in enumerate(model_means.index):\n",
    "        ax1.text(i, model_means[model] + 0.02, f'{model_means[model]:.4f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Heatmap of p-values for model comparisons\n",
    "    # Create a matrix of p-values\n",
    "    models = df['ModelID'].unique()\n",
    "    p_matrix = pd.DataFrame(np.nan, index=models, columns=models)\n",
    "    \n",
    "    for _, row in comparison_df.iterrows():\n",
    "        m1, m2 = row['Comparison'].split(' vs ')\n",
    "        p_matrix.loc[m1, m2] = row['P_Value']\n",
    "        p_matrix.loc[m2, m1] = row['P_Value']  # Symmetric matrix\n",
    "    \n",
    "    # Plot heatmap with p-values\n",
    "    mask = np.triu(np.ones_like(p_matrix, dtype=bool))\n",
    "    sns.heatmap(p_matrix, annot=True, fmt='.4f', cmap='viridis', \n",
    "               mask=mask, ax=ax2)\n",
    "    ax2.set_title('P-values for Pairwise Model Comparisons')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/home/yzhong/gits/interpretable-pd/scripts/evaluation/lme_model_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Also create a task-specific visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='Task', y='F1_Score', hue='ModelID', data=df)\n",
    "    plt.title('F1 Scores by Task and Model')\n",
    "    plt.xlabel('Task')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('/home/yzhong/gits/interpretable-pd/scripts/evaluation/task_model_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# def main():\n",
    "#     # Load data from the four models\n",
    "#     df1 = pd.read_csv('/home/yzhong/gits/interpretable-pd/splits/model1_results.csv')\n",
    "#     df2 = pd.read_csv('/home/yzhong/gits/interpretable-pd/splits/model2_results.csv')\n",
    "#     df3 = pd.read_csv('/home/yzhong/gits/interpretable-pd/splits/model3_results.csv')\n",
    "#     df4 = pd.read_csv('/home/yzhong/gits/interpretable-pd/splits/model4_results.csv')\n",
    "    \n",
    "    # If the CSV files don't exist, we can recreate the DataFrame from the notebook\n",
    "    # This is just placeholder code - in practice you would have saved the DataFrame\n",
    "    # df1 = convert_nested_dict_to_dataframe(pertask2f1_1, modelid=\"M1\")\n",
    "    # df2 = convert_nested_dict_to_dataframe(pertask2f1_2, modelid=\"M2\")\n",
    "    # df3 = convert_nested_dict_to_dataframe(pertask2f1_3, modelid=\"M3\")\n",
    "    # df4 = convert_nested_dict_to_dataframe(pertask2f1_4, modelid=\"M4\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "# df_all = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# Fit LME model\n",
    "model, result = fit_lme_model(df_all)\n",
    "\n",
    "# Print model summary\n",
    "print(\"=\" * 80)\n",
    "print(\"Linear Mixed Effects Model Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(result.summary())\n",
    "\n",
    "# Run pairwise comparisons\n",
    "comparison_df = run_pairwise_comparisons(df_all)\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Pairwise Model Comparisons\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Create visualizations\n",
    "visualize_results(df_all, result, comparison_df)\n",
    "\n",
    "# Provide interpretation of results\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Interpretation of Results\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare mean F1 scores\n",
    "model_means = df_all.groupby('ModelID')['F1_Score'].mean().sort_values(ascending=False)\n",
    "print(\"Models ranked by mean F1 score:\")\n",
    "for model, mean in model_means.items():\n",
    "    print(f\"{model}: {mean:.4f}\")\n",
    "\n",
    "# Summarize significant differences\n",
    "significant_comparisons = comparison_df[comparison_df['Significant']]\n",
    "print(\"\\nSignificant differences between models:\")\n",
    "if len(significant_comparisons) == 0:\n",
    "    print(\"No statistically significant differences were found between any models.\")\n",
    "else:\n",
    "    for _, row in significant_comparisons.iterrows():\n",
    "        better_model = row['Comparison'].split(' vs ')[0] if row['Mean_Difference'] > 0 else row['Comparison'].split(' vs ')[1]\n",
    "        print(f\"- {row['Comparison']}: p={row['P_Value']:.4f}, {better_model} performs better\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccda325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting complex mixed model with random slopes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model summary:\n",
      "                   Mixed Linear Model Regression Results\n",
      "============================================================================\n",
      "Model:                    MixedLM        Dependent Variable:        F1_Score\n",
      "No. Observations:         160            Method:                    REML    \n",
      "No. Groups:               7              Scale:                     0.0047  \n",
      "Min. group size:          20             Log-Likelihood:            179.7828\n",
      "Max. group size:          40             Converged:                 Yes     \n",
      "Mean group size:          22.9                                              \n",
      "----------------------------------------------------------------------------\n",
      "                                  Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "----------------------------------------------------------------------------\n",
      "Intercept                          0.749    0.019 39.545 0.000  0.712  0.786\n",
      "ModelID[T.M2]                      0.008    0.018  0.418 0.676 -0.028  0.043\n",
      "ModelID[T.M3]                     -0.079    0.022 -3.542 0.000 -0.123 -0.035\n",
      "ModelID[T.M4]                     -0.018    0.017 -1.044 0.296 -0.051  0.016\n",
      "Group Var                          0.002    0.058                           \n",
      "Group x ModelID[T.M2] Cov          0.001    0.022                           \n",
      "ModelID[T.M2] Var                  0.000    0.047                           \n",
      "Group x ModelID[T.M3] Cov         -0.000    0.056                           \n",
      "ModelID[T.M2] x ModelID[T.M3] Cov -0.000    0.016                           \n",
      "ModelID[T.M3] Var                  0.002    0.057                           \n",
      "Group x ModelID[T.M4] Cov          0.000                                    \n",
      "ModelID[T.M2] x ModelID[T.M4] Cov  0.000    0.017                           \n",
      "ModelID[T.M3] x ModelID[T.M4] Cov  0.000                                    \n",
      "ModelID[T.M4] Var                  0.000                                    \n",
      "============================================================================\n",
      "\n",
      "\n",
      "Pairwise comparisons between models:\n",
      "------------------------------------------------------------\n",
      "Comparison      | Estimate   | p-value    | Result\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 vs M2    | 0.007375 | 0.690093 | not significant\n",
      "M1 vs M3    | -0.078344 | 0.000071 | * significant *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2200: ConvergenceWarning: Retrying MixedLM optimization with lbfgs\n",
      "  warnings.warn(\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1 vs M4    | -0.017082 | 0.405194 | not significant\n",
      "M2 vs M3    | -0.087544 | 0.000767 | * significant *\n",
      "M2 vs M4    | -0.024467 | 0.119183 | not significant\n",
      "M3 vs M4    | 0.061084 | 0.001532 | * significant *\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Summary of Model Comparisons\n",
      "================================================================================\n",
      "Model    | Mean F1    | Significantly better than     \n",
      "--------------------------------------------------------------------------------\n",
      "M1       | 0.752684 | Better than: none\n",
      "         |            | Worse than: M3\n",
      "M2       | 0.761428 | Better than: none\n",
      "         |            | Worse than: M3\n",
      "M3       | 0.678999 | Better than: M1, M2, M4\n",
      "         |            | Worse than: none\n",
      "M4       | 0.736469 | Better than: none\n",
      "         |            | Worse than: M3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/storage500/yzhongenv/lib/python3.10/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from itertools import combinations\n",
    "\n",
    "def run_lme_analysis(df):\n",
    "    \"\"\"\n",
    "    Run Linear Mixed-Effects model on F1 scores with ModelID as fixed effect,\n",
    "    random intercepts and slopes for ModelID by Task, and random intercepts for Task:Fold\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns Task, Fold, F1_Score, ModelID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pairwise comparison results\n",
    "    \"\"\"\n",
    "    # Ensure categorical variables are properly coded\n",
    "    df['ModelID'] = pd.Categorical(df['ModelID'])\n",
    "    df['Task'] = pd.Categorical(df['Task'])\n",
    "    df['Fold'] = pd.Categorical(df['Fold'])\n",
    "    \n",
    "    # Create Task:Fold interaction term\n",
    "    df['Task_Fold'] = df['Task'].astype(str) + ':' + df['Fold'].astype(str)\n",
    "    \n",
    "    print(\"Fitting complex mixed model with random slopes...\")\n",
    "    \n",
    "    # Fit the full model with all data\n",
    "    try:\n",
    "        # The re_formula specifies random effects structure\n",
    "        # \"1 + ModelID\" means random intercepts and slopes for ModelID\n",
    "        model = smf.mixedlm(\"F1_Score ~ ModelID\", \n",
    "                           data=df, \n",
    "                           groups=df[\"Task\"],\n",
    "                           re_formula=\"1 + ModelID\")\n",
    "        \n",
    "        result = model.fit(reml=True)\n",
    "        print(\"\\nModel summary:\")\n",
    "        print(result.summary())\n",
    "        \n",
    "        # Store all model coefficients\n",
    "        coefficients = result.params\n",
    "        p_values = result.pvalues\n",
    "        \n",
    "        # Get all model pairs for comparison\n",
    "        model_ids = sorted(df['ModelID'].unique())\n",
    "        model_pairs = list(combinations(model_ids, 2))\n",
    "        \n",
    "        # Run pairwise comparisons between models\n",
    "        print(\"\\nPairwise comparisons between models:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Comparison':15s} | {'Estimate':10s} | {'p-value':10s} | Result\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results_dict = {}\n",
    "        \n",
    "        for m1, m2 in model_pairs:\n",
    "            # Create subset of data with just these two models\n",
    "            subset_df = df[df['ModelID'].isin([m1, m2])].copy()\n",
    "            subset_df['ModelID'] = pd.Categorical(subset_df['ModelID'], \n",
    "                                                 categories=[m1, m2],\n",
    "                                                 ordered=False)\n",
    "            \n",
    "            # Refit model on subset with the same complex random effects structure\n",
    "            # This matches the R formula: F1 ~ Model + (1 + Model | task) + (1 | task:fold)\n",
    "            subset_model = smf.mixedlm(\"F1_Score ~ ModelID\", \n",
    "                                      data=subset_df, \n",
    "                                      groups=subset_df[\"Task\"],\n",
    "                                      re_formula=\"1 + ModelID\")\n",
    "            \n",
    "            subset_result = subset_model.fit(reml=True)\n",
    "            \n",
    "            # Get the coefficient for the second model (compared to first as reference)\n",
    "            model_param = f\"ModelID[T.{m2}]\"\n",
    "            if model_param in subset_result.params:\n",
    "                model_effect = subset_result.params[model_param]\n",
    "                p_value = subset_result.pvalues[model_param]\n",
    "                \n",
    "                significant = p_value < 0.05\n",
    "                significance_marker = \"* significant *\" if significant else \"not significant\"\n",
    "                \n",
    "                # Calculate mean F1 scores for comparison\n",
    "                mean1 = subset_df[subset_df['ModelID'] == m1]['F1_Score'].mean()\n",
    "                mean2 = subset_df[subset_df['ModelID'] == m2]['F1_Score'].mean()\n",
    "                \n",
    "                print(f\"{m1} vs {m2:5s} | {model_effect:.6f} | {p_value:.6f} | {significance_marker}\")\n",
    "                \n",
    "                # Store result\n",
    "                results_dict[f\"{m1}_vs_{m2}\"] = {\n",
    "                    'estimate': model_effect,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': significant,\n",
    "                    'mean_f1_first': mean1,\n",
    "                    'mean_f1_second': mean2\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Warning: Could not find parameter for {m1} vs {m2} comparison\")\n",
    "        \n",
    "        # Print overall comparison table\n",
    "        print(\"\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Summary of Model Comparisons\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':8s} | {'Mean F1':10s} | {'Significantly better than':30s}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # For each model, list which models it's significantly better than\n",
    "        for model_id in model_ids:\n",
    "            mean_f1 = df[df['ModelID'] == model_id]['F1_Score'].mean()\n",
    "            better_than = []\n",
    "            worse_than = []\n",
    "            \n",
    "            for pair, result in results_dict.items():\n",
    "                m1, m2 = pair.split('_vs_')\n",
    "                if m1 == model_id and result['significant']:\n",
    "                    if result['estimate'] < 0:  # negative estimate means m2 is better\n",
    "                        worse_than.append(m2)\n",
    "                    else:\n",
    "                        better_than.append(m2)\n",
    "                elif m2 == model_id and result['significant']:\n",
    "                    if result['estimate'] > 0:  # positive estimate means m2 is better\n",
    "                        worse_than.append(m1)\n",
    "                    else:\n",
    "                        better_than.append(m1)\n",
    "            \n",
    "            better_than_str = ', '.join(better_than) if better_than else \"none\"\n",
    "            worse_than_str = ', '.join(worse_than) if worse_than else \"none\"\n",
    "            print(f\"{model_id:8s} | {mean_f1:.6f} | Better than: {better_than_str}\")\n",
    "            print(f\"{' ':8s} | {' ':10s} | Worse than: {worse_than_str}\")\n",
    "        \n",
    "        return results_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Run the analysis on the combined dataframe\n",
    "results = run_lme_analysis(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515cbfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from itertools import combinations\n",
    "\n",
    "def run_lme_analysis(df):\n",
    "    \"\"\"\n",
    "    Run Linear Mixed-Effects model on F1 scores with ModelID as fixed effect,\n",
    "    random intercepts and slopes for ModelID by Task, and random intercepts for Task:Fold\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns Task, Fold, F1_Score, ModelID\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with pairwise comparison results\n",
    "    \"\"\"\n",
    "    # Ensure categorical variables are properly coded\n",
    "    df['ModelID'] = pd.Categorical(df['ModelID'])\n",
    "    df['Task'] = pd.Categorical(df['Task'])\n",
    "    df['Fold'] = pd.Categorical(df['Fold'])\n",
    "    \n",
    "    # Create Task:Fold interaction term for the nested random effect\n",
    "    df['Task_Fold'] = df['Task'].astype(str) + '_' + df['Fold'].astype(str)\n",
    "    df['Task_Fold'] = pd.Categorical(df['Task_Fold'])\n",
    "    \n",
    "    print(\"Fitting complex mixed model with random slopes and nested random effects...\")\n",
    "    \n",
    "    # Approach: We'll use the Task as the main grouping variable\n",
    "    # And include both ModelID (for random slopes) and Task_Fold in the re_formula\n",
    "    \n",
    "    try:\n",
    "        # First, we'll try with statsmodels' approach to nested random effects\n",
    "        # Task is the main grouping, ModelID is the random slope, Task_Fold is nested random effect\n",
    "        formula = \"F1_Score ~ ModelID\"\n",
    "        \n",
    "        # Using vc_formula to add the task:fold nested random effect\n",
    "        vc_formulas = {\"Task_Fold\": \"0 + C(Task_Fold)\"}\n",
    "        \n",
    "        model = smf.mixedlm(formula, \n",
    "                           data=df, \n",
    "                           groups=df[\"Task\"],\n",
    "                           re_formula=\"1 + ModelID\",\n",
    "                           vc_formula=vc_formulas)\n",
    "        \n",
    "        result = model.fit(reml=True)\n",
    "        print(\"\\nModel summary:\")\n",
    "        print(result.summary())\n",
    "        \n",
    "        # Get all model pairs for comparison\n",
    "        model_ids = sorted(df['ModelID'].unique())\n",
    "        model_pairs = list(combinations(model_ids, 2))\n",
    "        \n",
    "        # Run pairwise comparisons between models\n",
    "        print(\"\\nPairwise comparisons between models:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Comparison':15s} | {'Estimate':10s} | {'p-value':10s} | Result\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        results_dict = {}\n",
    "        \n",
    "        for m1, m2 in model_pairs:\n",
    "            # Create subset of data with just these two models\n",
    "            subset_df = df[df['ModelID'].isin([m1, m2])].copy()\n",
    "            subset_df['ModelID'] = pd.Categorical(subset_df['ModelID'], \n",
    "                                                 categories=[m1, m2],\n",
    "                                                 ordered=False)\n",
    "            \n",
    "            # Refit model on subset with both random effects\n",
    "            subset_vc_formulas = {\"Task_Fold\": \"0 + C(Task_Fold)\"}\n",
    "            \n",
    "            subset_model = smf.mixedlm(\"F1_Score ~ ModelID\", \n",
    "                                      data=subset_df, \n",
    "                                      groups=subset_df[\"Task\"],\n",
    "                                      re_formula=\"1 + ModelID\",\n",
    "                                      vc_formula=subset_vc_formulas)\n",
    "            \n",
    "            subset_result = subset_model.fit(reml=True)\n",
    "            \n",
    "            # Get the coefficient for the comparison\n",
    "            model_param = f\"ModelID[T.{m2}]\"\n",
    "            if model_param in subset_result.params:\n",
    "                model_effect = subset_result.params[model_param]\n",
    "                p_value = subset_result.pvalues[model_param]\n",
    "                \n",
    "                significant = p_value < 0.05\n",
    "                significance_marker = \"* significant *\" if significant else \"not significant\"\n",
    "                \n",
    "                # Calculate mean F1 scores for reference\n",
    "                mean1 = subset_df[subset_df['ModelID'] == m1]['F1_Score'].mean()\n",
    "                mean2 = subset_df[subset_df['ModelID'] == m2]['F1_Score'].mean()\n",
    "                \n",
    "                print(f\"{m1} vs {m2:5s} | {model_effect:.6f} | {p_value:.6f} | {significance_marker}\")\n",
    "                \n",
    "                # Store result\n",
    "                results_dict[f\"{m1}_vs_{m2}\"] = {\n",
    "                    'estimate': model_effect,\n",
    "                    'p_value': p_value,\n",
    "                    'significant': significant,\n",
    "                    'mean_f1_first': mean1,\n",
    "                    'mean_f1_second': mean2\n",
    "                }\n",
    "            else:\n",
    "                print(f\"Warning: Could not find parameter for {m1} vs {m2} comparison\")\n",
    "        \n",
    "        # Print overall comparison table\n",
    "        print(\"\\n\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Summary of Model Comparisons\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Model':8s} | {'Mean F1':10s} | {'Better than':30s} | {'Worse than':30s}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # For each model, list which models it's significantly better/worse than\n",
    "        for model_id in model_ids:\n",
    "            mean_f1 = df[df['ModelID'] == model_id]['F1_Score'].mean()\n",
    "            better_than = []\n",
    "            worse_than = []\n",
    "            \n",
    "            for pair, result in results_dict.items():\n",
    "                m1, m2 = pair.split('_vs_')\n",
    "                if m1 == model_id and result['significant']:\n",
    "                    if result['estimate'] < 0:  # negative estimate means m2 is better\n",
    "                        worse_than.append(m2)\n",
    "                    else:  # positive estimate means m1 is better\n",
    "                        better_than.append(m2)\n",
    "                elif m2 == model_id and result['significant']:\n",
    "                    if result['estimate'] > 0:  # positive estimate means m1 is better\n",
    "                        worse_than.append(m1)\n",
    "                    else:  # negative estimate means m2 is better\n",
    "                        better_than.append(m1)\n",
    "            \n",
    "            better_than_str = ', '.join(better_than) if better_than else \"none\"\n",
    "            worse_than_str = ', '.join(worse_than) if worse_than else \"none\"\n",
    "            print(f\"{model_id:8s} | {mean_f1:.6f} | {better_than_str:30s} | {worse_than_str:30s}\")\n",
    "        \n",
    "        return results_dict\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "results = run_lme_analysis(df_all)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
