{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b287b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "def compute_average_report_across_runs(reports):\n",
    "    overall_report = {\n",
    "        'HC': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'PD': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'accuracy': [],\n",
    "        'macro avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "        'weighted avg': {'precision': [], 'recall': [], 'f1-score': [], 'support': []},\n",
    "    }\n",
    "\n",
    "    # Spoiler: It's gonna be inefficient :)\n",
    "\n",
    "    for report in reports:\n",
    "        for key in report.keys():\n",
    "            if key == 'accuracy':\n",
    "                overall_report[key].append(report[key])\n",
    "            else:\n",
    "                for key2 in report[key].keys():\n",
    "                    overall_report[key][key2].append(report[key][key2])\n",
    "\n",
    "    for key in overall_report.keys():\n",
    "            if key == 'accuracy':\n",
    "                overall_report[key] = f'{round(np.array(overall_report[key]).mean(), 4)}±{round(np.array(overall_report[key]).std(), 4)}'\n",
    "            else:\n",
    "                for key2 in report[key].keys():\n",
    "                    overall_report[key][key2] = f'{round(np.array(overall_report[key][key2]).mean(), 4)}±{round(np.array(overall_report[key][key2]).std(), 4)}'\n",
    "\n",
    "    # -- just for a more clean output\n",
    "    overall_report = pd.DataFrame.from_dict(overall_report).T\n",
    "    overall_report.iloc[2,0] = ''\n",
    "    overall_report.iloc[2,1] = ''\n",
    "    overall_report.iloc[2,3] = overall_report.iloc[3,3]\n",
    "\n",
    "    return overall_report\n",
    "\n",
    "def get_reports(exps_dir):\n",
    "    val_reports = []\n",
    "    test_reports = []\n",
    "    run_dirs = os.listdir(exps_dir)\n",
    "    for run_dir in run_dirs:\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        test_preds = []\n",
    "        test_labels = []\n",
    "\n",
    "        run_dir_path = os.path.join(exps_dir, run_dir)\n",
    "        fold_dirs = os.listdir(run_dir_path)\n",
    "        \n",
    "        for fold_dir in fold_dirs:\n",
    "            model_output_dir = os.path.join(run_dir_path, fold_dir, 'model_output')\n",
    "\n",
    "            # -- validation set\n",
    "            val_report_path = os.path.join(os.path.join(model_output_dir, 'validation_classification.pkl'))\n",
    "            with open(val_report_path, 'rb') as f:\n",
    "                val_model_output = pickle.load(f)\n",
    "\n",
    "            val_preds += val_model_output['preds']\n",
    "            val_labels += val_model_output['labels']\n",
    "\n",
    "            # -- test set\n",
    "            test_report_path = os.path.join(os.path.join(model_output_dir, 'test_classification.pkl'))\n",
    "            with open(test_report_path, 'rb') as f:\n",
    "                test_model_output = pickle.load(f)\n",
    "\n",
    "            test_preds += test_model_output['preds']\n",
    "            test_labels += test_model_output['labels']\n",
    "\n",
    "        # -- computing reports\n",
    "        val_reports.append(\n",
    "            classification_report(\n",
    "                val_labels,\n",
    "                val_preds,\n",
    "                target_names=['HC', 'PD'],\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        test_reports.append(\n",
    "            classification_report(\n",
    "                test_labels,\n",
    "                test_preds,\n",
    "                target_names=['HC', 'PD'],\n",
    "                output_dict=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return val_reports, test_reports\n",
    "\n",
    "\n",
    "def get_report_perfold(exps_dir):\n",
    "\n",
    "    val_fold_reports = {}\n",
    "    test_fold_reports = {}\n",
    "    run_dirs = sorted(os.listdir(exps_dir))\n",
    "    \n",
    "    for run_dir in run_dirs:\n",
    "        run_dir_path = os.path.join(exps_dir, run_dir)\n",
    "        fold_dirs = sorted(os.listdir(run_dir_path))\n",
    "        \n",
    "        val_fold_reports[run_dir] = []\n",
    "        test_fold_reports[run_dir] = []\n",
    "        \n",
    "        for fold_dir in fold_dirs:\n",
    "            model_output_dir = os.path.join(run_dir_path, fold_dir, 'model_output')\n",
    "            \n",
    "            # -- validation set\n",
    "            val_report_path = os.path.join(model_output_dir, 'validation_classification.pkl')\n",
    "            if os.path.exists(val_report_path):\n",
    "                with open(val_report_path, 'rb') as f:\n",
    "                    val_model_output = pickle.load(f)\n",
    "                \n",
    "                val_probs = [preds[1] for preds in val_model_output['probs']]\n",
    "                val_model_output['probs'] = val_probs  \n",
    "         \n",
    "                val_auc = roc_auc_score(val_model_output['labels'], val_probs)\n",
    "                \n",
    "                val_fold_report = classification_report(\n",
    "                    val_model_output['labels'],\n",
    "                    val_model_output['preds'],\n",
    "                    target_names=['HC', 'PD'],\n",
    "                    output_dict=True,\n",
    "                )\n",
    "                \n",
    "                val_fold_reports[run_dir].append({\n",
    "                    'fold': fold_dir,\n",
    "                    'report': val_fold_report,\n",
    "                    'preds': val_model_output['preds'],\n",
    "                    'labels': val_model_output['labels'],\n",
    "                    'probs': val_model_output['probs'],  \n",
    "                    'auc': val_auc  \n",
    "                })\n",
    "            \n",
    "            # -- test set\n",
    "            test_report_path = os.path.join(model_output_dir, 'test_classification.pkl')\n",
    "            if os.path.exists(test_report_path):\n",
    "                with open(test_report_path, 'rb') as f:\n",
    "                    test_model_output = pickle.load(f)\n",
    "                \n",
    "                test_probs = [preds[1] for preds in test_model_output['probs']]\n",
    "                test_model_output['probs'] = test_probs  \n",
    "                \n",
    "                test_auc = roc_auc_score(test_model_output['labels'], test_probs)\n",
    "                \n",
    "                test_fold_report = classification_report(\n",
    "                    test_model_output['labels'],\n",
    "                    test_model_output['preds'],\n",
    "                    target_names=['HC', 'PD'],\n",
    "                    output_dict=True,\n",
    "                )\n",
    "                \n",
    "                test_fold_reports[run_dir].append({\n",
    "                    'fold': fold_dir,\n",
    "                    'report': test_fold_report,\n",
    "                    'preds': test_model_output['preds'],\n",
    "                    'labels': test_model_output['labels'],\n",
    "                    'probs': test_model_output['probs'],  \n",
    "                    'auc': test_auc  \n",
    "                })\n",
    "    \n",
    "    return val_fold_reports, test_fold_reports\n",
    "\n",
    "def compute_fold_stats(fold_reports):\n",
    "\n",
    "    fold_stats = {}\n",
    "    \n",
    "    for run_dir, fold_results in fold_reports.items():\n",
    "        accuracies = [fold_result['report']['accuracy'] for fold_result in fold_results]\n",
    "        f1_scores = [fold_result['report']['PD']['f1-score'] for fold_result in fold_results]\n",
    "        precisions = [fold_result['report']['PD']['precision'] for fold_result in fold_results]\n",
    "        recalls = [fold_result['report']['PD']['recall'] for fold_result in fold_results]\n",
    "        \n",
    "        sensitivities = [fold_result['report']['PD']['recall'] for fold_result in fold_results]  \n",
    "        specificities = [fold_result['report']['HC']['recall'] for fold_result in fold_results] \n",
    "        \n",
    "        aucs = [fold_result['auc'] for fold_result in fold_results]\n",
    "        \n",
    "        fold_stats[run_dir] = {\n",
    "            'accuracy': {\n",
    "                'mean': np.mean(accuracies),\n",
    "                'std': np.std(accuracies)\n",
    "            },\n",
    "            'f1': {\n",
    "                'mean': np.mean(f1_scores),\n",
    "                'std': np.std(f1_scores)\n",
    "            },\n",
    "            'precision': {\n",
    "                'mean': np.mean(precisions),\n",
    "                'std': np.std(precisions)\n",
    "            },\n",
    "            'recall': {\n",
    "                'mean': np.mean(recalls),\n",
    "                'std': np.std(recalls)\n",
    "            },\n",
    "            'sensitivity': {\n",
    "                'mean': np.mean(sensitivities),\n",
    "                'std': np.std(sensitivities)\n",
    "            },\n",
    "            'specificity': {\n",
    "                'mean': np.mean(specificities),\n",
    "                'std': np.std(specificities)\n",
    "            },\n",
    "            'auc': {\n",
    "                'mean': np.mean(aucs),\n",
    "                'std': np.std(aucs)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return fold_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a8258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical comparison between: \n",
      "- Mcross_full and \n",
      "- Mallsent\n",
      "\n",
      "Wilcoxon Signed-Rank Test Results:\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_two_exps_with_wilcoxon(exps_dir1, exps_dir2):\n",
    "    \"\"\"\n",
    "    Compare results from two experiment directories using Wilcoxon signed-rank test\n",
    "    \n",
    "    Args:\n",
    "        exps_dir1: First experiment directory path\n",
    "        exps_dir2: Second experiment directory path\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with p-values for each metric\n",
    "    \"\"\"\n",
    "    # Get reports for both experiment directories\n",
    "    _, test_fold_reports1 = get_report_perfold(exps_dir1)\n",
    "    _, test_fold_reports2 = get_report_perfold(exps_dir2)\n",
    "    \n",
    "    # Store metrics per fold for both experiments\n",
    "    metrics_by_fold1 = defaultdict(lambda: defaultdict(list))\n",
    "    metrics_by_fold2 = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Process first experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports1.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold1[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold1[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold1[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold1[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold1[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    # Process second experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports2.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold2[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold2[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold2[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold2[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold2[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold2[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold2[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    # Calculate mean values for each fold\n",
    "    metrics_to_test = ['accuracy', 'f1', 'precision', 'sensitivity', 'specificity', 'auc']\n",
    "    fold_means1 = defaultdict(list)\n",
    "    fold_means2 = defaultdict(list)\n",
    "    \n",
    "    # Make sure we only use folds that exist in both experiments\n",
    "    common_folds = set(metrics_by_fold1.keys()) & set(metrics_by_fold2.keys())\n",
    "    \n",
    "    # Calculate means for each fold\n",
    "    for fold in common_folds:\n",
    "        for metric in metrics_to_test:\n",
    "            if metrics_by_fold1[fold][metric] and metrics_by_fold2[fold][metric]:\n",
    "                fold_means1[metric].append(np.mean(metrics_by_fold1[fold][metric]))\n",
    "                fold_means2[metric].append(np.mean(metrics_by_fold2[fold][metric]))\n",
    "    \n",
    "    # print(f\"Number of common folds: {len(common_folds)}\")\n",
    "    # print(f\"Metrics to test: {metrics_to_test}\")\n",
    "    # print(f\"Fold means for experiment 1: {fold_means1}\")\n",
    "    # print(f\"Fold means for experiment 2: {fold_means2}\")\n",
    "    \n",
    "    # Run Wilcoxon signed-rank test for each metric\n",
    "    results = {}\n",
    "    for metric in metrics_to_test:\n",
    "        if len(fold_means1[metric]) >= 5:  # Need at least 5 pairs for reliable results\n",
    "            statistic, p_value = stats.wilcoxon(fold_means1[metric], fold_means2[metric])\n",
    "            results[metric] = {\n",
    "                'p_value': p_value,\n",
    "                'significant': p_value < 0.05,\n",
    "                'means1': np.mean(fold_means1[metric]),\n",
    "                'means2': np.mean(fold_means2[metric]),\n",
    "                'difference': np.mean(fold_means1[metric]) - np.mean(fold_means2[metric])\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "exps_dir1 = \"/home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full/allsent/\"\n",
    "exps_dir2 = \"/home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full_fix_value/allsent/\"\n",
    "\n",
    "\n",
    "results = compare_two_exps_with_wilcoxon(exps_dir1, exps_dir2)\n",
    "\n",
    "# Print results in a nice format\n",
    "print(f\"Statistical comparison between: \\n- M{exps_dir1.strip('/').split('/')[-1]} and \\n- M{exps_dir2.strip('/').split('/')[-1]}\\n\")\n",
    "print(\"Wilcoxon Signed-Rank Test Results:\")\n",
    "for metric, result in results.items():\n",
    "    significance = \"* significant *\" if result['significant'] else \"not significant\"\n",
    "    direction = \">\" if result['difference'] > 0 else \"<\"\n",
    "    print(f\"{metric.upper()}: {result['means1']:.4f} {direction} {result['means2']:.4f}, p={result['p_value']:.4f} ({significance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b61138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full/allsent\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita-splitmono/cross_full/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full/DDK\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full/READ\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full/SENTENCES\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full/SUSTAINED-VOWELS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full/WORDS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full_fix/allsent\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita-splitmono/cross_full_fix/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix/DDK\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix/READ\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix/SENTENCES\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix/SUSTAINED-VOWELS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix/WORDS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_full_fix_value/allsent\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita-splitmono/cross_full_fix_value/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix_value/DDK\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix_value/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix_value/READ\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix_value/SENTENCES\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix_value/SUSTAINED-VOWELS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_full_fix_value/WORDS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/cross_token_oldcate/allsent\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita-splitmono/cross_token_oldcate/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_token_oldcate/DDK\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_token_oldcate/MONOLOGUE\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_token_oldcate/READ\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_token_oldcate/SENTENCES\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_token_oldcate/SUSTAINED-VOWELS\n",
      "Processing experiment directory: /home/yzhong/gits/interpretable-pd/exps/gita/cross_token_oldcate/WORDS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_avg_F1_for_allfolds(exps_dir1):\n",
    "    \"\"\"\n",
    "    Compare results from two experiment directories using Wilcoxon signed-rank test\n",
    "    \n",
    "    Args:\n",
    "        exps_dir1: First experiment directory path\n",
    "        exps_dir2: Second experiment directory path\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with p-values for each metric\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Processing experiment directory: {exps_dir1}\")\n",
    "    # Get reports for both experiment directories\n",
    "    _, test_fold_reports1 = get_report_perfold(exps_dir1)\n",
    "    \n",
    "    # Store metrics per fold for both experiments\n",
    "    metrics_by_fold1 = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # Process first experiment directory\n",
    "    for run_dir, fold_results in test_fold_reports1.items():\n",
    "        for fold_result in fold_results:\n",
    "            fold = fold_result['fold']\n",
    "            # Store metrics for this fold\n",
    "            metrics_by_fold1[fold]['accuracy'].append(fold_result['report']['accuracy'])\n",
    "            metrics_by_fold1[fold]['f1'].append(fold_result['report']['PD']['f1-score'])\n",
    "            metrics_by_fold1[fold]['precision'].append(fold_result['report']['PD']['precision'])\n",
    "            metrics_by_fold1[fold]['recall'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['sensitivity'].append(fold_result['report']['PD']['recall'])\n",
    "            metrics_by_fold1[fold]['specificity'].append(fold_result['report']['HC']['recall'])\n",
    "            metrics_by_fold1[fold]['auc'].append(fold_result['auc'])\n",
    "    \n",
    "    \n",
    "    # Calculate mean values for each fold\n",
    "    metrics_to_test = ['accuracy', 'f1', 'precision', 'sensitivity', 'specificity', 'auc']\n",
    "    fold_means1 = defaultdict(list)\n",
    "    \n",
    "    # Make sure we only use folds that exist in both experiments\n",
    "    common_folds = set(metrics_by_fold1.keys()) \n",
    "    fold_means1_f1 = []\n",
    "    # Calculate means for each fold\n",
    "    for fold in common_folds:\n",
    "\n",
    "        if metrics_by_fold1[fold]['f1'] :\n",
    "            fold_means1_f1.append(np.mean(metrics_by_fold1[fold]['f1']))\n",
    "            \n",
    "    return fold_means1_f1\n",
    "\n",
    "exp1 = \"cross_full\"\n",
    "exp2 = \"cross_full_fix\"\n",
    "exp3 = \"cross_full_fix_value\"\n",
    "exp4 = \"cross_token_oldcate\"\n",
    "exps = [exp1, exp2, exp3, exp4]\n",
    "\n",
    "def get_report_perexp(exp):\n",
    "    \n",
    "    dir1 = \"/home/yzhong/gits/interpretable-pd/exps/pcgita_splits_10foldnew/\"\n",
    "    dir2 = \"/home/yzhong/gits/interpretable-pd/exps/gita/\"\n",
    "    dir3 = \"/home/yzhong/gits/interpretable-pd/exps/gita-splitmono/\"\n",
    "    pertask2f1 = defaultdict(list)\n",
    "\n",
    "    exps_dir1 = os.path.join(dir1, exp, 'allsent') \n",
    "    fold_means_allsent = get_avg_F1_for_allfolds(exps_dir1)\n",
    "    pertask2f1['ALLSENT'] = fold_means_allsent\n",
    "    \n",
    "    exps_dir3 = os.path.join(dir3, exp, 'MONOLOGUE')\n",
    "    fold_means_splitmono = get_avg_F1_for_allfolds(exps_dir3)\n",
    "    pertask2f1['SPLIT-MONO'] = fold_means_splitmono\n",
    "\n",
    "    tasks = [\"DDK\", \"READ\", \"SENTENCES\", \"SUSTAINED-VOWELS\", \"WORDS\"]\n",
    "    for task in tasks:\n",
    "        exp_dir = os.path.join(dir2, exp, task)\n",
    "        if os.path.exists(exp_dir):\n",
    "            fold_means = get_avg_F1_for_allfolds(exp_dir)\n",
    "            pertask2f1[task] = fold_means\n",
    "            \n",
    "    \n",
    "    all_f1_scores = []\n",
    "    for task, f1_scores in pertask2f1.items():\n",
    "        all_f1_scores.extend(f1_scores)\n",
    "    return all_f1_scores\n",
    "\n",
    "\n",
    "all_f1_scores1 = get_report_perexp(exp1)\n",
    "all_f1_scores2 = get_report_perexp(exp2)\n",
    "all_f1_scores3 = get_report_perexp(exp3)\n",
    "all_f1_scores4 = get_report_perexp(exp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d581a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_f1_scores1: [0.8066321610566094, 0.8073277419212397, 0.8796157094167605, 0.7681825026598317, 0.7358288159801576, 0.8620941645579328, 0.767299895931475, 0.8319605098779521, 0.6552673804761117, 0.6335934482958153, 0.6809972608950107, 0.7854279216177625, 0.8132027789547422, 0.7710879355442068, 0.8318101590687089, 0.6996374175859857, 0.8371729179476668, 0.8126861639325738, 0.7863773522370272, 0.7137153977520951, 0.6923413703599461, 0.7729249011857708, 0.8463431305536568, 0.6378947368421052, 0.8944927536231884, 0.7487445887445888, 0.8691729323308269, 0.8188594014680971, 0.5259803921568628, 0.8404784688995216, 0.6243287314331025, 0.7945997368225334, 0.813085680922493, 0.784384973170349, 0.8663242677179357, 0.650103972108884, 0.7712133341307609, 0.7748921441994566, 0.7250658414255435, 0.7350179902467038]\n",
      "all_f1_scores2: [0.8142615365104608, 0.7873015586526346, 0.9202661847519436, 0.8033810448743395, 0.7738202916867264, 0.8870164133565528, 0.7842193310275445, 0.8334628210249871, 0.667503371999413, 0.653002278827531, 0.6690909028726173, 0.7563317198325562, 0.7800542568779834, 0.7810408595979403, 0.8337059120611141, 0.725401420102925, 0.7843769130478797, 0.8483137350221869, 0.7988991020668402, 0.6916272227691832, 0.7839598997493734, 0.7671223513328776, 0.8277861319966583, 0.7377443609022556, 0.7420289855072463, 0.8024189261031367, 0.8908878256246677, 0.8074824743016962, 0.7485448916408669, 0.8150895140664961, 0.7242281918505002, 0.7899382969971205, 0.8216908480117893, 0.785285657988832, 0.8601225439427431, 0.6671358142120745, 0.7610845189566561, 0.7760743971603115, 0.7714085217559103, 0.745328450052542]\n",
      "Comparison between M1 and M2: statistic=272.0, p-value=0.06414752090131515\n",
      "F1: 0.7667 < 0.7805, p=0.0641 (not significant)\n"
     ]
    }
   ],
   "source": [
    "# Perform Wilcoxon signed-rank test\n",
    "import scipy.stats as stats\n",
    "\n",
    "# turn to numpy arrays\n",
    "all_f1_scores1 = np.array(all_f1_scores1)\n",
    "all_f1_scores2 = np.array(all_f1_scores2)\n",
    "\n",
    "# Calculate mean values for comparison\n",
    "mean1 = np.mean(all_f1_scores1)\n",
    "mean2 = np.mean(all_f1_scores2)\n",
    "difference = mean1 - mean2\n",
    "\n",
    "# Perform the Wilcoxon test\n",
    "statistic1, p_value1 = stats.wilcoxon(all_f1_scores1, all_f1_scores2)\n",
    "\n",
    "# Create result dictionary for F1 score\n",
    "results = {\n",
    "    'f1': {\n",
    "        'p_value': p_value1,\n",
    "        'significant': p_value1 < 0.05,\n",
    "        'means1': mean1,\n",
    "        'means2': mean2,\n",
    "        'difference': difference\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Comparison between M{1} and M{2}: statistic={statistic1}, p-value={p_value1}\")\n",
    "\n",
    "for metric, result in results.items():\n",
    "    significance = \"* significant *\" if result['significant'] else \"not significant\"\n",
    "    direction = \">\" if result['difference'] > 0 else \"<\"\n",
    "    print(f\"{metric.upper()}: {result['means1']:.4f} {direction} {result['means2']:.4f}, p={result['p_value']:.4f} ({significance})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ccda325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Wilcoxon Signed-Rank Test Results for All Model Combinations\n",
      "================================================================================\n",
      "Comparison      | Mean 1   | Mean 2   | Diff     | p-value    | Result\n",
      "--------------------------------------------------------------------------------\n",
      "M1 vs M2    | 0.7545 | 0.7626 | -0.0081 | 0.400848 | not significant\n",
      "M1 vs M3    | 0.7545 | 0.6685 | 0.0860 | 0.000000 | * significant *\n",
      "M1 vs M4    | 0.7545 | 0.7285 | 0.0260 | 0.000717 | * significant *\n",
      "M2 vs M3    | 0.7626 | 0.6685 | 0.0941 | 0.000000 | * significant *\n",
      "M2 vs M4    | 0.7626 | 0.7285 | 0.0341 | 0.000010 | * significant *\n",
      "M3 vs M4    | 0.6685 | 0.7285 | -0.0600 | 0.000000 | * significant *\n",
      "================================================================================\n",
      "\n",
      "P-value Matrix (row vs column):\n",
      "        M1      M2      M3      M4      \n",
      "M1      ---     0.4008480.0000000.000717\n",
      "M2      0.400848---     0.0000000.000010\n",
      "M3      0.0000000.000000---     0.000000\n",
      "M4      0.0007170.0000100.000000---     \n"
     ]
    }
   ],
   "source": [
    "def compare_all_model_combinations(all_f1_scores_dict):\n",
    "    \"\"\"\n",
    "    Compare all possible combinations of models using Wilcoxon signed-rank test\n",
    "    \n",
    "    Args:\n",
    "        all_f1_scores_dict: Dictionary with model names as keys and F1 score arrays as values\n",
    "        \n",
    "    Returns:\n",
    "        results_dict: Dictionary with pairwise comparison results\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    \n",
    "    # Convert all arrays to numpy arrays if not already\n",
    "    for key in all_f1_scores_dict:\n",
    "        all_f1_scores_dict[key] = np.array(all_f1_scores_dict[key])\n",
    "    \n",
    "    # Get all possible combinations of model pairs\n",
    "    model_pairs = list(itertools.combinations(all_f1_scores_dict.keys(), 2))\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results_dict = {}\n",
    "    \n",
    "    # Run Wilcoxon test on all pairs\n",
    "    for model1, model2 in model_pairs:\n",
    "        scores1 = all_f1_scores_dict[model1]\n",
    "        scores2 = all_f1_scores_dict[model2]\n",
    "        \n",
    "        # Calculate mean values for comparison\n",
    "        mean1 = np.mean(scores1)\n",
    "        mean2 = np.mean(scores2)\n",
    "        difference = mean1 - mean2\n",
    "        \n",
    "        # Perform the Wilcoxon test\n",
    "        statistic, p_value = stats.wilcoxon(scores1, scores2)\n",
    "        \n",
    "        # Store results\n",
    "        pair_key = f\"{model1}_vs_{model2}\"\n",
    "        results_dict[pair_key] = {\n",
    "            'statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'means1': mean1,\n",
    "            'means2': mean2,\n",
    "            'difference': difference\n",
    "        }\n",
    "    \n",
    "    return results_dict\n",
    "\n",
    "# Create a dictionary with your F1 scores\n",
    "all_f1_scores_dict = {\n",
    "    'M1': all_f1_scores1,\n",
    "    'M2': all_f1_scores2,\n",
    "    'M3': all_f1_scores3,\n",
    "    'M4': all_f1_scores4\n",
    "}\n",
    "\n",
    "# Run the comparison\n",
    "results = compare_all_model_combinations(all_f1_scores_dict)\n",
    "\n",
    "# Print results in a nicely formatted table\n",
    "print(\"=\" * 80)\n",
    "print(\"Wilcoxon Signed-Rank Test Results for All Model Combinations\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Comparison':15s} | {'Mean 1':8s} | {'Mean 2':8s} | {'Diff':8s} | {'p-value':10s} | Result\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for pair, result in results.items():\n",
    "    model1, model2 = pair.split('_vs_')\n",
    "    significance = \"* significant *\" if result['significant'] else \"not significant\"\n",
    "    direction = \">\" if result['difference'] > 0 else \"<\"\n",
    "    print(f\"{model1} vs {model2:5s} | {result['means1']:.4f} | {result['means2']:.4f} | {result['difference']:.4f} | {result['p_value']:.6f} | {significance}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a matrix of p-values for visualization\n",
    "print(\"\\nP-value Matrix (row vs column):\")\n",
    "models = sorted(all_f1_scores_dict.keys())\n",
    "print(f\"{'':8s}\", end=\"\")\n",
    "for model in models:\n",
    "    print(f\"{model:8s}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for model1 in models:\n",
    "    print(f\"{model1:8s}\", end=\"\")\n",
    "    for model2 in models:\n",
    "        if model1 == model2:\n",
    "            print(f\"{'---':8s}\", end=\"\")\n",
    "        else:\n",
    "            pair_key = f\"{model1}_vs_{model2}\" if f\"{model1}_vs_{model2}\" in results else f\"{model2}_vs_{model1}\"\n",
    "            if pair_key in results:\n",
    "                p_value = results[pair_key]['p_value']\n",
    "                print(f\"{p_value:.6f}\", end=\"\")\n",
    "            else:\n",
    "                print(f\"{'N/A':8s}\", end=\"\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yzhongenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
